{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "helper_download_det_dataset",
      "provenance": [],
      "collapsed_sections": [
        "QGmHHVbfbihO",
        "W1ONjA-vRIxj"
      ],
      "mount_file_id": "12ZWCWIdxeUSw0F5y6WUiGDc6mbI-kV24",
      "authorship_tag": "ABX9TyM0PHdvz9JJD5C2UIvSCe27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balandongiv/mmocr_tutorial/blob/main/helper_download_det_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "yxLc0Q53PzvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "import os\n",
        "from os.path import exists,isfile\n",
        "import requests\n",
        "import shutil\n",
        "import logging\n",
        "import wget\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logging.info('test')"
      ],
      "metadata": {
        "id": "rBEylsXZDCpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\n",
        "\n",
        "# Install mmdetection\n",
        "!pip install mmdet\n",
        "\n",
        "# # Install mmocr\n",
        "!git clone https://github.com/open-mmlab/mmocr.git\n",
        "%cd mmocr\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e ."
      ],
      "metadata": {
        "id": "QAtwe5Xi0-yX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803c80af-d75f-4006-f87c-e2fd18b0644f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\n",
            "Collecting mmcv-full\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/mmcv_full-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (38.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.0 MB 6.4 MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (21.3)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (4.1.2.30)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full) (3.0.9)\n",
            "Installing collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.5.3 yapf-0.32.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mmdet\n",
            "  Downloading mmdet-2.25.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmdet) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mmdet) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from mmdet) (2.0.4)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmdet) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mmdet) (4.2.0)\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "Successfully installed mmdet-2.25.0 terminaltables-3.1.10\n",
            "Cloning into 'mmocr'...\n",
            "remote: Enumerating objects: 6714, done.\u001b[K\n",
            "remote: Counting objects: 100% (647/647), done.\u001b[K\n",
            "remote: Compressing objects: 100% (484/484), done.\u001b[K\n",
            "remote: Total 6714 (delta 306), reused 348 (delta 153), pack-reused 6067\u001b[K\n",
            "Receiving objects: 100% (6714/6714), 13.30 MiB | 19.01 MiB/s, done.\n",
            "Resolving deltas: 100% (4073/4073), done.\n",
            "/content/mmocr\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements/build.txt (line 2)) (1.21.6)\n",
            "Collecting pyclipper\n",
            "  Downloading pyclipper-1.3.0.post3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (604 kB)\n",
            "\u001b[K     |████████████████████████████████| 604 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements/build.txt (line 4)) (1.11.0+cu113)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.7/dist-packages (from -r requirements/runtime.txt (line 1)) (0.2.9)\n",
            "Collecting lanms-neo==1.0.2\n",
            "  Downloading lanms_neo-1.0.2.tar.gz (39 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from -r requirements/runtime.txt (line 3)) (0.99)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements/runtime.txt (line 4)) (3.2.2)\n",
            "Collecting opencv-python!=4.5.5.*,>=4.2.0.32\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.9 MB 111 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from -r requirements/runtime.txt (line 8)) (2.0.4)\n",
            "Collecting rapidfuzz>=2.0.0\n",
            "  Downloading rapidfuzz-2.0.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r requirements/runtime.txt (line 10)) (0.18.3)\n",
            "Collecting asynctest\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting codecov\n",
            "  Downloading codecov-2.1.12-py2.py3-none-any.whl (16 kB)\n",
            "Collecting flake8\n",
            "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting isort\n",
            "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting kwarray\n",
            "  Downloading kwarray-0.6.2-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements/tests.txt (line 7)) (3.6.4)\n",
            "Collecting pytest-cov\n",
            "  Downloading pytest_cov-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting pytest-runner\n",
            "  Downloading pytest_runner-6.0.0-py3-none-any.whl (7.2 kB)\n",
            "Collecting ubelt\n",
            "  Downloading ubelt-1.1.1-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 56.6 MB/s \n",
            "\u001b[?25hCollecting xdoctest>=0.10.0\n",
            "  Downloading xdoctest-1.0.0-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from -r requirements/tests.txt (line 12)) (0.32.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->-r requirements/build.txt (line 4)) (4.2.0)\n",
            "Collecting jarowinkler<1.1.0,>=1.0.2\n",
            "  Downloading jarowinkler-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from xdoctest>=0.10.0->-r requirements/tests.txt (line 11)) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug->-r requirements/runtime.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug->-r requirements/runtime.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug->-r requirements/runtime.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug->-r requirements/runtime.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements/runtime.txt (line 10)) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements/runtime.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements/runtime.txt (line 10)) (2021.11.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements/runtime.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements/runtime.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements/runtime.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements/runtime.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from codecov->-r requirements/tests.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from codecov->-r requirements/tests.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements/tests.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements/tests.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements/tests.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->-r requirements/tests.txt (line 2)) (2022.5.18.1)\n",
            "Collecting pyflakes<2.5.0,>=2.4.0\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.9.0,>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 988 kB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting importlib-metadata<4.3\n",
            "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->flake8->-r requirements/tests.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (21.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (8.13.0)\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting coverage[toml]>=5.2.1\n",
            "  Downloading coverage-6.4.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (21.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements/tests.txt (line 7)) (2.0.1)\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 71.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 48.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 71.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.0.0-py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 59.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pytest\n",
            "  Downloading pytest-6.2.4-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 60.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 59.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.2-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 54.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 56.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.0-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 70.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 66.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.1-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 56.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.0-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 57.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.2-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 49.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 59.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.0-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 37.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.3-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 51.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.2-py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 45.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.1-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 57.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.0-py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 49.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.5-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 59.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 60.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.3-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 60.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.2-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 70.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.1-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 38.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.0-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 60.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.4-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.3-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 59.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.2-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 49.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.1-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 59.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.0-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 58.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 59.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.2-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 50.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.1-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 55.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.0-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 42.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.0.1-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 57.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.0.0-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 58.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.11-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 58.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.10-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 58.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.9-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 50.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.8-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 68.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.7-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 49.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.6-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 59.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.5-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 75.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.4-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 62.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.3-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 73.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.2-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 51.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.1-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 72.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 53.9 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-cov to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-cov\n",
            "  Downloading pytest_cov-2.12.1-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.12.0-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.11.0-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.10.1-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading pytest_cov-2.10.0-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading pytest_cov-2.9.0-py2.py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: lanms-neo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dw(sfile,url,wget_dw=False):\n",
        "  if not isfile(sfile):\n",
        "      logging.info(f\"Downloading {os.path.split(sfile)[-1]} from\"\n",
        "                     f\" {url}.\")\n",
        "      if wget_dw:\n",
        "        wget.download(url, out=sfile)\n",
        "      else:\n",
        "        r = requests.get(url, verify=False,stream=True)  \n",
        "        with open(sfile, 'wb') as f:\n",
        "          f.write(r.content)\n",
        "def ch_make_folder(f):\n",
        "  if not os.path.exists(f):\n",
        "    os.makedirs(f)\n",
        "\n",
        "def move_files(source_dir,dest):\n",
        "  file_names = os.listdir(source_dir)\n",
        "  for file_name in file_names:\n",
        "    shutil.move(os.path.join(source_dir, file_name),dest)\n",
        "\n",
        "def move_files_to_des(file_names,dest):\n",
        "    for file_name in file_names:\n",
        "        shutil.move(file_name ,dest)"
      ],
      "metadata": {
        "id": "EoCtOL7dfnTz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICDAR 2011 (Born-Digital Images)\n"
      ],
      "metadata": {
        "id": "KcyfqRoFQgaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Recognition"
      ],
      "metadata": {
        "id": "bOo7ozKyTIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def icdar2011(npath,cleanup=False):\n",
        "    \n",
        "    root=os.path.join(npath,'icdar2011')\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "    dcrops=os.path.join(root,'crops')\n",
        "    dpath=dict(tr_img=dict(URL = \"https://rrc.cvc.uab.es/downloads/Challenge1_Training_Task3_Images_GT.zip\",\n",
        "                           fpath=os.path.join(root,'Challenge1_Training_Task3_Images_GT.zip')),\n",
        "               ts_img=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge1_Test_Task3_Images.zip',\n",
        "                           fpath=os.path.join(root,'Challenge1_Test_Task3_Images.zip')),\n",
        "               ts_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge1_Test_Task3_GT.txt',\n",
        "                           fpath=os.path.join(root,'Challenge1_Test_Task3_GT.txt'))\n",
        "               )\n",
        "\n",
        "\n",
        "\n",
        "    for dp in ([root,dcrops,dannot]):\n",
        "        ch_make_folder(dp)\n",
        "\n",
        "    for dp in (['tr_img','ts_img','ts_lbl']):\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "    dimg_tr=os.path.join(dcrops,'train')\n",
        "    dimg_ts=os.path.join(dcrops,'test')\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "\n",
        "    for dp, dirc_f in zip(['tr_img','ts_img'],\n",
        "                          [dimg_tr,dimg_ts]):\n",
        "        logging.info(f\"Unpacking {dpath[dp]['fpath']} to {dirc_f}. \")\n",
        "        shutil.unpack_archive(dpath[dp]['fpath'],dirc_f)\n",
        "\n",
        "\n",
        "    logging.info(f'Move the annotation')\n",
        "    fannot_ts=os.path.join(dannot,'Challenge1_Test_Task3_GT.txt')\n",
        "    shutil.move(dpath['ts_lbl']['fpath'],fannot_ts)\n",
        "\n",
        "\n",
        "    shutil.move(os.path.join(dimg_tr,'gt.txt'),\n",
        "                os.path.join(dannot,'Challenge1_Train_Task3_GT.txt'))\n",
        "\n",
        "    # Text Recognition\n",
        "    from tools.data.textrecog.ic11_converter import convert_annotations\n",
        "    format = 'jsonl'\n",
        "    for split in ['Train', 'Test']:\n",
        "        convert_annotations(root, split, format)\n",
        "        logging.info(f'{split} split converted.')\n",
        "\n",
        "    #     ├── icdar2011\n",
        "    # │   ├── crops\n",
        "    # │   ├── train_label.jsonl\n",
        "    # │   └── test_label.jsonl\n",
        "\n",
        "\n",
        "    ## Text Detection\n",
        "    if cleanup:\n",
        "        logging.info ('Cleaning up')\n",
        "        for dp in ['tr_img','ts_img']:\n",
        "            os.remove(dpath[dp]['fpath'])\n",
        "\n",
        "\n",
        "icdar2011('/content')"
      ],
      "metadata": {
        "id": "-12UAvM4JFvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Detection\n",
        "ICDAR 2011 (Born-Digital Images)"
      ],
      "metadata": {
        "id": "m2biMYC3OCLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def icdar2011(npath,cleanup=False):\n",
        "    \n",
        "    root=os.path.join(npath,'icdar2011')\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "    dcrops=os.path.join(root,'imgs')\n",
        "    dpath=dict(tr_img=dict(URL = \"https://rrc.cvc.uab.es/downloads/Challenge1_Training_Task12_Images.zip\",\n",
        "                           fpath=os.path.join(root,'Challenge1_Training_Task12_Images.zip')),\n",
        "               tr_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge1_Training_Task1_GT.zip',\n",
        "                           fpath=os.path.join(root,'Challenge1_Training_Task1_GT.zip')),\n",
        "               ts_img=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge1_Test_Task12_Images.zip',\n",
        "                           fpath=os.path.join(root,'Challenge1_Test_Task12_Images.zip')),\n",
        "               ts_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge1_Test_Task1_GT.zip',\n",
        "                           fpath=os.path.join(root,'Challenge1_Test_Task1_GT.zip')),\n",
        "               )\n",
        "\n",
        "\n",
        "    ch_make_folder(root)\n",
        "\n",
        "    for dp in (['tr_img','ts_img','ts_lbl','tr_lbl']):\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "    dimg_tr=os.path.join(dcrops,'training')\n",
        "    dimg_ts=os.path.join(dcrops,'test')\n",
        "    lbl_tr=os.path.join(dannot,'training')\n",
        "    lbl_ts=os.path.join(dannot,'test')\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "    for dp, dirc_f in zip(['tr_img','ts_img','tr_lbl','ts_lbl'],\n",
        "                          [dimg_tr,dimg_ts,lbl_tr,lbl_ts]):\n",
        "        logging.info(f\"Unpacking {dpath[dp]['fpath']} to {dirc_f}. \")\n",
        "        shutil.unpack_archive(dpath[dp]['fpath'],dirc_f)\n",
        "\n",
        "    ## Text Detection\n",
        "\n",
        "\n",
        "    #     │── icdar2011\n",
        "    # │   ├── imgs\n",
        "    # │   ├── instances_test.json\n",
        "    # │   └── instances_training.json\n",
        "\n",
        "\n",
        "    if cleanup:\n",
        "        logging.info ('Cleaning up')\n",
        "        for dp in ['tr_img','ts_img']:\n",
        "            os.remove(dpath[dp]['fpath'])\n",
        "\n",
        "\n",
        "icdar2011('/content/detect/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-BxpPo5OOKT",
        "outputId": "8298fe9f-703a-4805-f6cf-1435d08d367f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Unpacking file\n",
            "INFO:root:Unpacking /content/detect/icdar2011/Challenge1_Training_Task12_Images.zip to /content/detect/icdar2011/img/training. \n",
            "INFO:root:Unpacking /content/detect/icdar2011/Challenge1_Test_Task12_Images.zip to /content/detect/icdar2011/img/test. \n",
            "INFO:root:Unpacking /content/detect/icdar2011/Challenge1_Training_Task1_GT.zip to /content/detect/icdar2011/annotations/training. \n",
            "INFO:root:Unpacking /content/detect/icdar2011/Challenge1_Test_Task1_GT.zip to /content/detect/icdar2011/annotations/test. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Generate instances_training.json and instances_test.json with the following command:\n",
        "\n",
        "`python tools/data/textdet/ic11_converter.py PATH/TO/icdar2011 --nproc 4`"
      ],
      "metadata": {
        "id": "J4X8IvhGR6Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import mmcv\n",
        "from PIL import Image\n",
        "\n",
        "from mmocr.utils import convert_annotations\n",
        "from tools.data.textdet.ic11_converter import collect_files,collect_annotations\n",
        "nproc=10\n",
        "\n",
        "root_path ='/content/detect/icdar2011'\n",
        "\n",
        "for split in ['training', 'test']:\n",
        "    print(f'Processing {split} set...')\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert annotation'):\n",
        "        files = collect_files(\n",
        "            osp.join(root_path, 'imgs', split),\n",
        "            osp.join(root_path, 'annotations', split))\n",
        "        image_infos = collect_annotations(files, nproc=nproc)\n",
        "        convert_annotations(\n",
        "            image_infos, osp.join(root_path,\n",
        "                                  'instances_' + split + '.json'))"
      ],
      "metadata": {
        "id": "Xvwth9zAMCYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICDAR 2013 (Focused Scene Text)"
      ],
      "metadata": {
        "id": "U4-3tHmoQkt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Recognition"
      ],
      "metadata": {
        "id": "pp8iFQ7hTun6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def icdar2013(npath,cleanup=False):\n",
        "    root=os.path.join(npath,'icdar2013')\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "    dcrops=os.path.join(root,'crops')\n",
        "    dpath=dict(tr_img=dict(URL = \"https://rrc.cvc.uab.es/downloads/Challenge2_Training_Task3_Images_GT.zip\",\n",
        "                           fpath=os.path.join(root,'Challenge2_Training_Task3_Images_GT.zip')),\n",
        "               ts_img=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge2_Test_Task3_Images.zip',\n",
        "                           fpath=os.path.join(root,'Challenge2_Test_Task3_Images.zip')),\n",
        "               ts_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge2_Test_Task3_GT.txt',\n",
        "                           fpath=os.path.join(root,'Challenge2_Test_Task3_GT.txt'))\n",
        "               )\n",
        "\n",
        "    for dp in ([root,dcrops,dannot]):\n",
        "        ch_make_folder(dp)\n",
        "\n",
        "    for dp in (['tr_img','ts_img','ts_lbl']):\n",
        "        logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "    dimg_tr=os.path.join(dcrops,'train')\n",
        "    dimg_ts=os.path.join(dcrops,'test')\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "\n",
        "    for dp, dirc_f in zip(['tr_img','ts_img'],\n",
        "                          [dimg_tr,dimg_ts]):\n",
        "        logging.info(f\"Unpacking {dpath[dp]['fpath']} to {dirc_f}. \")\n",
        "        shutil.unpack_archive(dpath[dp]['fpath'],dirc_f)\n",
        "\n",
        "\n",
        "    logging.info(f'Move the annotation')\n",
        "    fannot_ts=os.path.join(dannot,'Challenge2_Test_Task3_GT.txt')\n",
        "    shutil.move(dpath['ts_lbl']['fpath'],fannot_ts)\n",
        "\n",
        "    fannot_from=os.path.join(dimg_tr,'gt.txt')\n",
        "    fannot_tr=os.path.join(dannot,'Challenge2_Train_Task3_GT.txt')\n",
        "    shutil.move(fannot_from,fannot_tr)\n",
        "\n",
        "    format = 'jsonl'\n",
        "    from tools.data.textrecog.ic13_converter import convert_annotations\n",
        "    for split in ['Train', 'Test']:\n",
        "        convert_annotations(root, split, format)\n",
        "        print(f'{split} split converted.')\n",
        "\n",
        "\n",
        "    if cleanup:\n",
        "        logging.info ('Cleaning up')\n",
        "        for dp in ['tr_img','ts_img']:\n",
        "            os.remove(dpath[dp]['fpath'])\n",
        "\n",
        "icdar2013('/content')"
      ],
      "metadata": {
        "id": "bp_6_MwYxRLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Detection"
      ],
      "metadata": {
        "id": "yL1RQFcpT15n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def icdar2013(npath,cleanup=False):\n",
        "    root=os.path.join(npath,'icdar2013')\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "    dcrops=os.path.join(root,'imgs')\n",
        "    dpath=dict(tr_img=dict(URL = \"https://rrc.cvc.uab.es/downloads/Challenge2_Training_Task12_Images.zip\",\n",
        "                           fpath=os.path.join(root,'Challenge2_Training_Task12_Images.zip')),\n",
        "               ts_img=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge2_Test_Task12_Images.zip',\n",
        "                           fpath=os.path.join(root,'Challenge2_Test_Task12_Images.zip')),\n",
        "               tr_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge2_Training_Task1_GT.zip',\n",
        "                           fpath=os.path.join(root,'Challenge2_Training_Task1_GT.zip')),\n",
        "               ts_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge2_Test_Task1_GT.zip',\n",
        "                           fpath=os.path.join(root,'Challenge2_Test_Task1_GT.zip')),\n",
        "               )\n",
        "\n",
        "\n",
        "    ch_make_folder(root)\n",
        "\n",
        "    for dp in (['tr_img','ts_img','ts_lbl','tr_lbl']):\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "    dimg_tr=os.path.join(dcrops,'training')\n",
        "    dimg_ts=os.path.join(dcrops,'test')\n",
        "    lbl_tr=os.path.join(dannot,'training')\n",
        "    lbl_ts=os.path.join(dannot,'test')\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "    for dp, dirc_f in zip(['tr_img','ts_img','tr_lbl','ts_lbl'],\n",
        "                          [dimg_tr,dimg_ts,lbl_tr,lbl_ts]):\n",
        "        logging.info(f\"Unpacking {dpath[dp]['fpath']} to {dirc_f}. \")\n",
        "        shutil.unpack_archive(dpath[dp]['fpath'],dirc_f)\n",
        "\n",
        "icdar2013('/content/detect')"
      ],
      "metadata": {
        "id": "Zgkl8wpsyOzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Generate instances_training.json and instances_test.json with the following command:\n",
        "\n",
        "`python tools/data/textdet/ic13_converter.py PATH/TO/icdar2013 --nproc 4`\n"
      ],
      "metadata": {
        "id": "PoAlSZg0VSIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import mmcv\n",
        "from PIL import Image\n",
        "\n",
        "from mmocr.utils import convert_annotations\n",
        "from tools.data.textdet.ic13_converter import collect_files,collect_annotations\n",
        "nproc=10\n",
        "\n",
        "\n",
        "root_path ='/content/detect/icdar2013'\n",
        "\n",
        "for split in ['training', 'test']:\n",
        "    print(f'Processing {split} set...')\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert IC13 annotation'):\n",
        "        files = collect_files(\n",
        "            osp.join(root_path, 'imgs', split),\n",
        "            osp.join(root_path, 'annotations', split), split)\n",
        "        image_infos = collect_annotations(files, nproc=nproc)\n",
        "        convert_annotations(\n",
        "            image_infos, osp.join(root_path,\n",
        "                                  'instances_' + split + '.json'))"
      ],
      "metadata": {
        "id": "Uy2ohSSvVhtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# icdar2015"
      ],
      "metadata": {
        "id": "leY9vVAsQnO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Recognition"
      ],
      "metadata": {
        "id": "2nL7I6sGWRjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def icdar2015(npath,cleanup=False):\n",
        "  \"\"\"\n",
        "    \n",
        "    Remark: icdar2015 does not have special python  converter\n",
        "    :param npath: \n",
        "    :param cleanup: \n",
        "    :return: \n",
        "  \"\"\"\n",
        "\n",
        "  root=os.path.join(npath,'icdar2015')\n",
        "  dts=os.path.join(root,'ch4_test_word_images_gt')\n",
        "  dtr=os.path.join(root,'ch4_training_word_images_gt')\n",
        "  dpath=dict(tr_img=dict(URL = \"https://rrc.cvc.uab.es/downloads/ch4_training_word_images_gt.zip\",\n",
        "                           fpath=os.path.join(root,'ch4_training_word_images_gt.zip')),\n",
        "               ts_img=dict(URL='https://rrc.cvc.uab.es/downloads/ch4_test_word_images_gt.zip',\n",
        "                           fpath=os.path.join(root,'ch4_test_word_images_gt.zip')),\n",
        "               # ts_lblx=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge4_Test_Task3_GT.txt',\n",
        "               #              fpath=os.path.join(root,'Challenge4_Test_Task3_GT.txt')),\n",
        "               ts_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/icdar_2015/test_label.txt',\n",
        "                           fpath=os.path.join(root,'test_label.txt')),\n",
        "               tr_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/icdar_2015/train_label.txt',\n",
        "                           fpath=os.path.join(root,'train_label.txt')),\n",
        "               )\n",
        "\n",
        "\n",
        "  for dp in ([root,dts,dtr]):\n",
        "      ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  for dp in (['tr_img','ts_img','ts_lbl','tr_lbl']):\n",
        "      logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "      check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  for dp, dirc_f in zip(['tr_img','ts_img'],\n",
        "                          [dtr,dts]):\n",
        "      logging.info(f\"Unpacking {dpath[dp]['fpath']} to {dirc_f}. \")\n",
        "      shutil.unpack_archive(dpath[dp]['fpath'],dirc_f)\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "      logging.info ('Cleaning up')\n",
        "      for dp in ['tr_img','ts_img']:\n",
        "          os.remove(dpath[dp]['fpath'])\n",
        "\n",
        "  #  ```\n",
        "  #   text\n",
        "  # ├── icdar2015\n",
        "  # │   ├── imgs\n",
        "  # │   ├── annotations\n",
        "  # │   ├── instances_test.json\n",
        "  # │   └── instances_training.json\n",
        "  # ```\n",
        "  icdar2015('/content')"
      ],
      "metadata": {
        "id": "ObU9jsZf0quq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s6LN2gHP0s7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Detection"
      ],
      "metadata": {
        "id": "fIQvcnJBWWps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def icdar2015(npath,cleanup=False):\n",
        "    root=os.path.join(npath,'icdar2015')\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "    dcrops=os.path.join(root,'imgs')\n",
        "\n",
        "    dpath=dict(tr_img=dict(URL = \"https://rrc.cvc.uab.es/downloads/ch4_training_images.zip\",\n",
        "                           fpath=os.path.join(root,'ch4_training_images.zip')),\n",
        "               tr_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/ch4_training_localization_transcription_gt.zip',\n",
        "                           fpath=os.path.join(root,'ch4_test_images.zip')),\n",
        "               ts_img=dict(URL='https://rrc.cvc.uab.es/downloads/ch4_test_images.zip',\n",
        "                           fpath=os.path.join(root,'ch4_training_localization_transcription_gt.zip')),\n",
        "               ts_lbl=dict(URL='https://rrc.cvc.uab.es/downloads/Challenge4_Test_Task1_GT.zip',\n",
        "                           fpath=os.path.join(root,'Challenge4_Test_Task1_GT.zip'))\n",
        "               )\n",
        "\n",
        "\n",
        "    ch_make_folder(root)\n",
        "\n",
        "    for dp in (['tr_img','ts_img','ts_lbl','tr_lbl']):\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "    dimg_tr=os.path.join(dcrops,'training')\n",
        "    dimg_ts=os.path.join(dcrops,'test')\n",
        "    lbl_tr=os.path.join(dannot,'training')\n",
        "    lbl_ts=os.path.join(dannot,'test')\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "    for dp, dirc_f in zip(['tr_img','ts_img','tr_lbl','ts_lbl'],\n",
        "                          [dimg_tr,dimg_ts,lbl_tr,lbl_ts]):\n",
        "        logging.info(f\"Unpacking {dpath[dp]['fpath']} to {dirc_f}. \")\n",
        "        shutil.unpack_archive(dpath[dp]['fpath'],dirc_f)\n",
        "\n",
        "icdar2015('/content/detect')"
      ],
      "metadata": {
        "id": "oIljBOp6WZ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " generate instances_training.json and instances_test.json with the following command:\n",
        "\n",
        "`python tools/data/textdet/icdar_converter.py /path/to/icdar2015 -o /path/to/icdar2015 -d icdar2015 --split-list training test`\n"
      ],
      "metadata": {
        "id": "QWbrxyhaYsAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Why got to may ignore text?\n",
        "\"\"\"\n",
        "\n",
        "import os.path as osp\n",
        "import mmcv\n",
        "from tools.data.textdet.icdar_converter import collect_files,collect_annotations\n",
        "\n",
        "icdar_path = '/content/detect/icdar2015'\n",
        "out_dir= '/content/detect/icdar2015'\n",
        "dataset='icdar2015'\n",
        "split_list=['training', 'test']\n",
        "nproc=10\n",
        "out_dir = out_dir if out_dir else icdar_path\n",
        "mmcv.mkdir_or_exist(out_dir)\n",
        "\n",
        "img_dir = osp.join(icdar_path, 'imgs')\n",
        "gt_dir = osp.join(icdar_path, 'annotations')\n",
        "\n",
        "set_name = {}\n",
        "for split in split_list:\n",
        "    set_name.update({split: 'instances_' + split + '.json'})\n",
        "    assert osp.exists(osp.join(img_dir, split))\n",
        "\n",
        "for split, json_name in set_name.items():\n",
        "    print(f'Converting {split} into {json_name}')\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert icdar annotation'):\n",
        "        files = collect_files(\n",
        "            osp.join(img_dir, split), osp.join(gt_dir, split))\n",
        "        image_infos = collect_annotations(\n",
        "            files, dataset, nproc=nproc)\n",
        "        convert_annotations(image_infos, osp.join(out_dir, json_name))"
      ],
      "metadata": {
        "id": "tT9qWPKWYopc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IIIT5K"
      ],
      "metadata": {
        "id": "voUV5XqhQ53O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Recognition"
      ],
      "metadata": {
        "id": "QGmHHVbfbihO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IIIT5K(npath,cleanup=False):\n",
        "\n",
        "    root=os.path.join(npath,'IIIT5K')\n",
        "    dpath=dict(dt_img=dict(URL = \"http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K-Word_V3.0.tar.gz\",\n",
        "                           fpath=os.path.join(root,'IIIT5K-Word_V3.0.tar.gz')),\n",
        "               ts_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/IIIT5K/test_label.txt',\n",
        "                           fpath=os.path.join(root,'test_label.txt')),\n",
        "               tr_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/IIIT5K/train_label.txt',\n",
        "                           fpath=os.path.join(root,'train_label.txt')),\n",
        "               )\n",
        "\n",
        "    ch_make_folder(root)\n",
        "\n",
        "    for dp in (['dt_img','ts_lbl','tr_lbl']):\n",
        "        logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "    shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "    logging.info(f'Moving file')\n",
        "    ftest_from=os.path.join(root,'IIIT5K','test')\n",
        "    shutil.move(ftest_from,root)\n",
        "\n",
        "    ftrain_from=os.path.join(root,'IIIT5K','train')\n",
        "    shutil.move(ftrain_from,root)\n",
        "\n",
        "    if cleanup:\n",
        "        logging.info ('Cleaning up')\n",
        "        shutil.rmtree(os.path.join(root,'IIIT5K'))\n",
        "        os.remove(dpath['dt_img']['fpath'])\n",
        "\n",
        "    # ├── III5K\n",
        "    # │   ├── train_label.txt\n",
        "    # │   ├── test_label.txt\n",
        "    # │   ├── train\n",
        "    # │   └── test\n",
        "\n",
        "IIIT5K('/content')\n"
      ],
      "metadata": {
        "id": "qzboG71yQ39c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af44d30-0844-44ee-d9fb-f4e35cf87816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Downloading IIIT5K-Word_V3.0.tar.gz from http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K-Word_V3.0.tar.gz.\n",
            "INFO:root:Downloading IIIT5K-Word_V3.0.tar.gz from http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K-Word_V3.0.tar.gz.\n",
            "INFO:root:Downloading test_label.txt from https://download.openmmlab.com/mmocr/data/mixture/IIIT5K/test_label.txt.\n",
            "INFO:root:Downloading test_label.txt from https://download.openmmlab.com/mmocr/data/mixture/IIIT5K/test_label.txt.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading train_label.txt from https://download.openmmlab.com/mmocr/data/mixture/IIIT5K/train_label.txt.\n",
            "INFO:root:Downloading train_label.txt from https://download.openmmlab.com/mmocr/data/mixture/IIIT5K/train_label.txt.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Unpacking file\n",
            "INFO:root:Moving file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVT"
      ],
      "metadata": {
        "id": "U1B4-rwpWQNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Recognition"
      ],
      "metadata": {
        "id": "i2Bt59crihyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def list_to_file(filename, lines):\n",
        "    \"\"\"Write a list of strings to a text file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The output filename. It will be created/overwritten.\n",
        "        lines (list(str)): Data to be written.\n",
        "    \"\"\"\n",
        "    import mmcv\n",
        "    mmcv.mkdir_or_exist(os.path.dirname(filename))\n",
        "    with open(filename, 'w', encoding='utf-8') as fw:\n",
        "        for line in lines:\n",
        "            fw.write(f'{line}\\n')\n",
        "            \n",
        "def svt(npath,cleanup=False,height=32,width=100,resize=False):\n",
        "    \"\"\"\n",
        "    parser.add_argument('--height', default=32, help='Resize height.')\n",
        "    parser.add_argument('--width', default=100, help='Resize width.')\n",
        "    :param npath:\n",
        "    :param cleanup:\n",
        "    :param height:\n",
        "    :param width:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    import os.path as osp\n",
        "    import xml.etree.ElementTree as ET\n",
        "    import cv2\n",
        "    root=os.path.join(npath,'svt')\n",
        "\n",
        "    dimg=os.path.join(root,'image')\n",
        "    dpath=dict(dt_img=dict(URL = \"http://www.iapr-tc11.org/dataset/SVT/svt.zip\",\n",
        "                           fpath=os.path.join(root,'svt.zip')),\n",
        "               ts_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/svt/test_label.txt',\n",
        "                           fpath=os.path.join(root,'test_label.txt'))\n",
        "               )\n",
        "\n",
        "\n",
        "    for dp in ([root,dimg]):\n",
        "        ch_make_folder(dp)\n",
        "\n",
        "    for dp in (['dt_img','ts_lbl']):\n",
        "        logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "        check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "    logging.info(f'Unpacking file')\n",
        "    shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "    # inputs\n",
        "    root_path=os.path.join(root,'svt1')\n",
        "    src_label_file = osp.join(root_path, 'test.xml')\n",
        "    if not osp.exists(src_label_file):\n",
        "        raise Exception(\n",
        "            f'{src_label_file} not exists, please check and try again.')\n",
        "    src_image_root = root_path\n",
        "\n",
        "    # outputs\n",
        "    dst_label_file = osp.join(root, 'test_label.txt')\n",
        "    dst_image_root = dimg\n",
        "\n",
        "    tree = ET.parse(src_label_file)\n",
        "    rootxmls = tree.getroot()\n",
        "\n",
        "    index = 1\n",
        "    lines = []\n",
        "    total_img_num = len(rootxmls)\n",
        "    i = 1\n",
        "    for image_node in rootxmls.findall('image'):\n",
        "        image_name = image_node.find('imageName').text\n",
        "        # print(f'[{i}/{total_img_num}] Process image: {image_name}')\n",
        "        i += 1\n",
        "        lexicon = image_node.find('lex').text.lower()\n",
        "        lexicon_list = lexicon.split(',')\n",
        "        lex_size = len(lexicon_list)\n",
        "        src_img = cv2.imread(osp.join(src_image_root, image_name))\n",
        "        for rectangle in image_node.find('taggedRectangles'):\n",
        "            x = int(rectangle.get('x'))\n",
        "            y = int(rectangle.get('y'))\n",
        "            w = int(rectangle.get('width'))\n",
        "            h = int(rectangle.get('height'))\n",
        "            rb, re = max(0, y), max(0, y + h)\n",
        "            cb, ce = max(0, x), max(0, x + w)\n",
        "            dst_img = src_img[rb:re, cb:ce]\n",
        "            text_label = rectangle.find('tag').text.lower()\n",
        "            if resize:\n",
        "                dst_img = cv2.resize(dst_img, (width, height))\n",
        "            dst_img_name = f'img_{index:04}' + '.jpg'\n",
        "            index += 1\n",
        "            dst_img_path = osp.join(dst_image_root, dst_img_name)\n",
        "            cv2.imwrite(dst_img_path, dst_img)\n",
        "            lines.append(f'{osp.basename(dst_image_root)}/{dst_img_name} '\n",
        "                         f'{text_label} {lex_size} {lexicon}')\n",
        "\n",
        "\n",
        "    list_to_file(dst_label_file, lines)\n",
        "    print(f'Finish to generate svt testset, '\n",
        "          f'with label file {dst_label_file}')\n",
        "\n",
        "    if cleanup:\n",
        "            logging.info ('Cleaning up')\n",
        "            shutil.rmtree(os.path.join(root,'__MACOSX'))\n",
        "            shutil.rmtree(os.path.join(root,'svt1'))\n",
        "            os.remove(dpath['dt_img']['fpath'])\n",
        "#   ├── svt\n",
        "# │   ├── test_label.txt\n",
        "# │   └── image\n",
        "svt('/content')"
      ],
      "metadata": {
        "id": "lEy2JWDoUy7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://msvocds.blob.core.windows.net/coco2014/train2014.zip --no-check-certificate"
      ],
      "metadata": {
        "id": "95on4NtNU7Cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9087e6-abd3-46b7-a87b-d77900fff79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 14:34:10--  http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
            "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 20.60.195.163\n",
            "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|20.60.195.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/octet-stream]\n",
            "Saving to: ‘train2014.zip’\n",
            "\n",
            "train2014.zip         9%[>                   ]   1.16G  4.46MB/s    eta 24m 48s^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COCO-Text"
      ],
      "metadata": {
        "id": "L_d-nrEXWahs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ICDAR2017 Robust Reading Challenge on COCO-Text"
      ],
      "metadata": {
        "id": "G19ki2Rui8lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#coco-text)\n"
      ],
      "metadata": {
        "id": "fZQ4Is9hjDnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coco_text(npath,cleanup=False):\n",
        "  root=os.path.join(npath,'coco_text')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dcrops=os.path.join(root,'crops')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://datasets.cvc.uab.es/rrc/COCO-Text-words-trainval.zip\",\n",
        "                         fpath=os.path.join(root,'COCO-Text-words-trainval.zip')),\n",
        "             tr_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/coco_text/train_label.txt',\n",
        "                         fpath=os.path.join(root,'train_label.txt'))\n",
        "             )\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  for dp in ([root,dcrops,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "  \n",
        "  for dp in (['dt_img','tr_lbl']):\n",
        "    logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  dimg_tr=os.path.join(dcrops,'train')\n",
        "  dimg_ts=os.path.join(dcrops,'test')\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "  if cleanup:\n",
        "      logging.info ('Cleaning up')\n",
        "      os.remove(dpath['dt_img']['fpath'])\n",
        "\n",
        "\n",
        "coco_text('/content')"
      ],
      "metadata": {
        "id": "41Emp5cEWdIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b39e03f-3b6f-47d4-cda3-440cb4f81066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Downloading COCO-Text-words-trainval.zip from https://datasets.cvc.uab.es/rrc/COCO-Text-words-trainval.zip.\n",
            "INFO:root:Downloading train_label.txt from https://download.openmmlab.com/mmocr/data/mixture/coco_text/train_label.txt.\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Detection (KIV) [link text](https://mmocr.readthedocs.io/en/latest/datasets/det.html#icdar-2017)\n",
        "\n",
        "\n",
        "\n",
        "I am still confuse where to download the images.zip file. Is it from this [link](https://rrc.cvc.uab.es/?ch=5&com=downloads)?"
      ],
      "metadata": {
        "id": "sV_asd7Yjjjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, generate instances_training.json and instances_test.json with the following command:\n",
        "\n",
        "`python tools/data/textdet/icdar_converter.py /path/to/icdar2015 -o /path/to/icdar2015 -d icdar2015 --split-list training test`\n"
      ],
      "metadata": {
        "id": "qWUvoHg1lvkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MJSynth (Syn90k)"
      ],
      "metadata": {
        "id": "HcskSXOXbiX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#mjsynth-syn90k)\n",
        "\n",
        "KIV since the file is to large"
      ],
      "metadata": {
        "id": "KolHbOGfmE9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Syn90k(npath,cleanup=False):\n",
        "  root=os.path.join(npath,'Syn90k')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dcrops=os.path.join(root,'crops')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://thor.robots.ox.ac.uk/~vgg/data/text/mjsynth.tar.gz\",\n",
        "                         fpath=os.path.join(root,'mjsynth.tar.gz')),\n",
        "             shfl_lbl=dict(URL = \"https://download.openmmlab.com/mmocr/data/mixture/Syn90k/shuffle_labels.txt\",\n",
        "                         fpath=os.path.join(root,'shuffle_labels.txt')),\n",
        "             ts_lbl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/Syn90k/label.txt',\n",
        "                         fpath=os.path.join(root,'label.txt'))\n",
        "             )\n",
        "  \n",
        "\n",
        "  # if not isfile(dpath['dt_img']['fpath']):\n",
        "  #   raise (f\"Please download the file at {dpath['dt_img']['fpath']} and save the file in {root}\")\n",
        "\n",
        "\n",
        "  \n",
        "  for dp in ([root,dcrops,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info(f'This going to take a very long time to download. 9.98 Gb')\n",
        "  for dp in (['dt_img','ts_lbl','shfl_lbl']):\n",
        "    logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "\n",
        "  # python tools/data/utils/txt2lmdb.py -i data/mixture/Syn90k/label.txt -o data/mixture/Syn90k/label.lmdb\n",
        "#   ├── Syn90k\n",
        "# │   ├── shuffle_labels.txt\n",
        "# │   ├── label.txt\n",
        "# │   ├── label.lmdb (optional)\n",
        "# │   └── mnt\n",
        "Syn90k('/content')"
      ],
      "metadata": {
        "id": "wQ_eE3TkbqLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [SynthText](https://mmocr.readthedocs.io/en/latest/datasets/det.html#synthtext)\n",
        "\n",
        "Yet  to test since the file is damn large\n",
        "\n",
        "\n",
        "Overview\n",
        "\n",
        "This is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout.\n",
        "\n",
        "The dataset consists of 800 thousand images with approximately 8 million synthetic word instances. Each text instance is annotated with its text-string, word-level and character-level bounding-boxes. "
      ],
      "metadata": {
        "id": "0zCGkEl2mdqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SynthText (Synth800k)\n",
        "KIV since the file is to large\n",
        "\n",
        "\n",
        "Overview\n",
        "\n",
        "This is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout.\n",
        "\n",
        "The dataset consists of 800 thousand images with approximately 8 million synthetic word instances. Each text instance is annotated with its text-string, word-level and character-level bounding-boxes. "
      ],
      "metadata": {
        "id": "va4hWZU96Rge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#synthtext-synth800k)"
      ],
      "metadata": {
        "id": "uIqP0Hmtp55Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SynthText(npath,cleanup=False):\n",
        "  root=os.path.join(npath,'SynthText')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://thor.robots.ox.ac.uk/~vgg/data/scenetext/SynthText.zip\",\n",
        "                         fpath=os.path.join(root,'SynthText.zip')),\n",
        "             lbl_txt=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/SynthText/label.txt',\n",
        "                         fpath=os.path.join(root,'label.txt')),\n",
        "             lbl_shfl=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/SynthText/shuffle_labels.txt',\n",
        "                         fpath=os.path.join(root,'shuffle_labels.txt')),\n",
        "             lbl_tr=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/SynthText/instances_train.txt',\n",
        "                         fpath=os.path.join(root,'instances_train'))\n",
        "             )\n",
        "  \n",
        "  ch_make_folder(root)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for dp in (['lbl_shfl','lbl_txt','lbl_tr']):\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "    raise (f\"This file is large which is about 38Gb,Please download the file at {dpath['dt_img']['fpath']} and save the file in {root}\\\n",
        "    Further instruction on how to download the dataset can be found at https://www.robots.ox.ac.uk/~vgg/data/scenetext/\")\n",
        "\n",
        "\n",
        "# ├── SynthText\n",
        "# │   ├── alphanumeric_labels.txt\n",
        "# │   ├── shuffle_labels.txt\n",
        "# │   ├── instances_train.txt\n",
        "# │   ├── label.txt\n",
        "# │   ├── label.lmdb (optional)\n",
        "# │   └── synthtext\n",
        "  "
      ],
      "metadata": {
        "id": "djCXHshbf7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#synthtext)"
      ],
      "metadata": {
        "id": "3vPW8mcOptW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synthtext(npath,cleanup=False):\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  The SynthText dataset (74 GiB) is available for download via BitTorrent from Academic Torrents. \n",
        "  This includes the pre-generated dataset as well as the pre-processed background images.\n",
        "\n",
        "  We strongly recommend the use of BitTorrent protocol. For when that is not possible, \n",
        "  the pre-generated dataset (38 GiB) is available for download over http. For instructions\n",
        "   on how to download the pre-processed background images over http, see the SynthText project on github.\n",
        "\n",
        "  \"\"\"\n",
        "  root=os.path.join(npath,'Syn90k')\n",
        "  # dannot=os.path.join(root,'annotations')\n",
        "  dcrops=os.path.join(root,'imgs')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://thor.robots.ox.ac.uk/~vgg/data/scenetext/SynthText.zip\",\n",
        "                         fpath=os.path.join(root,'SynthText.zip')),\n",
        "             lbl_dta=dict(URL = \"https://download.openmmlab.com/mmocr/data/synthtext/instances_training.lmdb/data.mdb\",\n",
        "                         fpath=os.path.join(root,'data.mdb')),\n",
        "             lbl_loc=dict(URL='https://download.openmmlab.com/mmocr/data/synthtext/instances_training.lmdb/lock.mdb',\n",
        "                         fpath=os.path.join(root,'lock.mdb'))\n",
        "             )\n",
        "  \n",
        "\n",
        "  # if not isfile(dpath['dt_img']['fpath']):\n",
        "  #   raise (f\"Please download the file at {dpath['dt_img']['fpath']} and save the file in {root}\")\n",
        "\n",
        "\n",
        "  \n",
        "  for dp in ([root,dcrops]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info(f'This going to take a very long time to download. 38 Gb')\n",
        "  for dp in (['dt_img','lbl_loc','lbl_dta']):\n",
        "    logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],dcrops)\n",
        "\n",
        "\n",
        "  # ├── synthtext\n",
        "  # │   ├── imgs\n",
        "  # │   └── instances_training.lmdb\n",
        "  # │       ├── data.mdb\n",
        "  # │       └── lock.mdb\n",
        "synthtext('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXeeSgobc5lV",
        "outputId": "9ae863e3-e744-4ded-a44e-5bcc5efc26ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This going to take a very long time to download\n",
            "INFO:root:The file /content/Syn90k/dds.txt is not availaible, downloading from https://download.openmmlab.com/mmocr/data/mixture/Syn90k/label.txt\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SynthText('/content')"
      ],
      "metadata": {
        "id": "vdxbGErn8Hyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SynthAdd (KIV)\n",
        "\n",
        "KIV SINCE THE WEBSITE IS IN CHINESE"
      ],
      "metadata": {
        "id": "pNn-07bP9-X1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#synthadd)"
      ],
      "metadata": {
        "id": "-KM3Mm7-rDcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SynthAdd(npath):\n",
        "  # TODO: REQUIRE LOGIN IN CHINESE\n",
        "  root=os.path.join(npath,'SynthAdd')\n",
        "  # dannot=os.path.join(root,'annotations')\n",
        "  # dcrops=os.path.join(root,'crops')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://thor.robots.ox.ac.uk/~vgg/data/scenetext/SynthText.zip\",\n",
        "                         fpath=os.path.join(root,'SynthText.zip')),\n",
        "             lbl_txt=dict(URL='https://download.openmmlab.com/mmocr/data/mixture/SynthAdd/label.txt',\n",
        "                         fpath=os.path.join(root,'label.txt')),\n",
        "             )\n",
        "  \n",
        "  ch_make_folder(root)\n",
        "\n",
        "\n",
        "  check_dw(dpath['lbl_txt']['fpath'],dpath['lbl_txt']['URL'])\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "    raise (f\"This file is large which is about 38Gb,Please download the file at {dpath['dt_img']['fpath']} and save the file in {root}\\\n",
        "    Further instruction on how to download the dataset can be found at https://www.robots.ox.ac.uk/~vgg/data/scenetext/\")\n",
        "\n",
        "SynthAdd('/content')"
      ],
      "metadata": {
        "id": "3ZXaEgax-BOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mrz6L0uf-Gmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip --no-check-certificate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czDXKuc42Bss",
        "outputId": "1c2d0b46-ca0b-48e3-a3a0-e580770751a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-13 02:17:44--  https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7072297970 (6.6G) [application/zip]\n",
            "Saving to: ‘train_val_images.zip’\n",
            "\n",
            "train_val_images.zi 100%[===================>]   6.59G  61.8MB/s    in 98s     \n",
            "\n",
            "2022-06-13 02:19:22 (68.9 MB/s) - ‘train_val_images.zip’ saved [7072297970/7072297970]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TextOCR"
      ],
      "metadata": {
        "id": "9BIGMHeK-cGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#textocr)"
      ],
      "metadata": {
        "id": "To3Zm93Yrhc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip --no-check-certificate"
      ],
      "metadata": {
        "id": "cWYSrGZBJkfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textocr(npath):\n",
        "  root=os.path.join(npath,'textocr')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\",\n",
        "                         fpath=os.path.join(root,'train_val_images.zip')),\n",
        "             lbl_tr=dict(URL='https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_train.json',\n",
        "                         fpath=os.path.join(root,'TextOCR_0.1_train.json')),\n",
        "             lbl_val=dict(URL='https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_val.json',\n",
        "                         fpath=os.path.join(root,'TextOCR_0.1_val.json')),\n",
        "             )\n",
        "  \n",
        "  ch_make_folder(root)\n",
        "\n",
        "  # Downloading the train_val_images.zip in Google Colab might have an issue whereby the session crashed after using all available RAM. One way to bypass is to use\n",
        "  # !wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip --no-check-certificate --continue \n",
        "  # and move the train_val_images.zip onto root.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for dp in (['lbl_tr','lbl_val']):\n",
        "    logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "                     f\" {dpath[dp]['URL']}.\")\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "      raise ValueError('No file available. !wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip --no-check-certificate --continue ')\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "\n",
        "  os.rename(os.path.join(root,'train_images'),\n",
        "            os.path.join(root,'train'))\n",
        "\n",
        "  from tools.data.textrecog.textocr_converter import convert_textocr\n",
        "  \n",
        "  n_proc=10 # Utilise parallel processing\n",
        "  num_train_imgs = convert_textocr(\n",
        "          root_path=root_path,\n",
        "          dst_image_path='image',\n",
        "          dst_label_filename='train_label.txt',\n",
        "          annotation_filename='TextOCR_0.1_train.json',\n",
        "          nproc=n_proc)\n",
        "\n",
        "\n",
        "  print(f'Total number of the training images: {num_train_imgs}')\n",
        "  print('Processing validation set...')\n",
        "  convert_textocr(\n",
        "          root_path=root_path,\n",
        "          dst_image_path='image',\n",
        "          dst_label_filename='val_label.txt',\n",
        "          annotation_filename='TextOCR_0.1_val.json',\n",
        "          img_start_idx=num_train_imgs,\n",
        "          nproc=n_proc)\n",
        "\n",
        "  #   ├── TextOCR\n",
        "  # │   ├── image\n",
        "  # │   ├── train_label.txt\n",
        "  # │   └── val_label.txt\n",
        "textocr('/content')"
      ],
      "metadata": {
        "id": "mJwPZHbK-ft6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#textocr)\n",
        "\n",
        "File preparation for the `text detection` is similar to `text recognition` except for creating the mmocr-compatible annotation file"
      ],
      "metadata": {
        "id": "llmbop83sD-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textocr(npath):\n",
        "  root=os.path.join(npath,'textocr')\n",
        "  dpath=dict(dt_img=dict(URL = \"https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\",\n",
        "                         fpath=os.path.join(root,'train_val_images.zip')),\n",
        "             lbl_tr=dict(URL='https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_train.json',\n",
        "                         fpath=os.path.join(root,'TextOCR_0.1_train.json')),\n",
        "             lbl_val=dict(URL='https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_val.json',\n",
        "                         fpath=os.path.join(root,'TextOCR_0.1_val.json')),\n",
        "             )\n",
        "\n",
        "  ch_make_folder(root)\n",
        "\n",
        "  # Downloading the train_val_images.zip in Google Colab might have an issue whereby the session crashed after using all available RAM. One way to bypass is to use\n",
        "  # !wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip --no-check-certificate --continue \n",
        "  # and move the train_val_images.zip onto root.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for dp in (['dt_img','lbl_tr','lbl_val']):\n",
        "    # logging.info(f\"Downloading {os.path.split(dpath[dp]['fpath'])[-1]} from\"\n",
        "    #                  f\" {dpath[dp]['URL']}.\")\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'],wget_dw=True)\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "      raise ValueError('No file available. !wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip --no-check-certificate --continue ')\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "\n",
        "  os.rename(os.path.join(root,'train_images'),\n",
        "            os.path.join(root,'train'))\n",
        "\n",
        "\n",
        "  from tools.data.textdet.textocr_converter import collect_textocr_info,convert_annotations\n",
        "  import os.path as osp\n",
        "  root_path = '/content/detection/textocr'\n",
        "  print('Processing training set...')\n",
        "  training_infos = collect_textocr_info(root_path, 'TextOCR_0.1_train.json')\n",
        "  convert_annotations(training_infos,\n",
        "                      osp.join(root_path, 'instances_training.json'))\n",
        "  print('Processing validation set...')\n",
        "  val_infos = collect_textocr_info(root_path, 'TextOCR_0.1_val.json')\n",
        "  convert_annotations(val_infos, osp.join(root_path, 'instances_val.json'))\n",
        "  print('Finish')\n",
        "\n",
        "\n",
        "  # The resulting directory structure looks like the following:\n",
        "\n",
        "  # ├── textocr\n",
        "  # │   ├── train\n",
        "  # │   ├── instances_training.json\n",
        "  # │   └── instances_val.json\n",
        "\n",
        "textocr('/content/detection/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN2MjXTWS8IX",
        "outputId": "ebdc9700-3d6b-41e2-a95a-0f14f39b3eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Downloading train_val_images.zip from https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip.\n",
            "INFO:root:Downloading train_val_images.zip from https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip.\n",
            "INFO:root:Downloading TextOCR_0.1_train.json from https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_train.json.\n",
            "INFO:root:Downloading TextOCR_0.1_train.json from https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_train.json.\n",
            "INFO:root:Downloading TextOCR_0.1_val.json from https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_val.json.\n",
            "INFO:root:Downloading TextOCR_0.1_val.json from https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_val.json.\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CuPsvhivVEvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Totaltext"
      ],
      "metadata": {
        "id": "E75M1JBL_StK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#totaltext)"
      ],
      "metadata": {
        "id": "-k092iMMskro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "Asopq3-4BTos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def totaltext(npath,cleanup=False):\n",
        "  import gdown\n",
        "  # TODO: REQUIRE LOGIN IN CHINESE\n",
        "  root=os.path.join(npath,'totaltext')\n",
        "  dannot=os.path.join(root,'annotations','training')\n",
        "  dcrops=os.path.join(root,'imgs')\n",
        "\n",
        "\n",
        "  dpath=dict(dt_img=dict(URL = 'https://drive.google.com/open?id=1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2&authuser=0',\n",
        "                         fpath=os.path.join(root,'totaltext.zip')),\n",
        "             lbl_tr=dict(URL='https://drive.google.com/open?id=1-XrQBoU9as1PXaB_0dUrDTJgvGFFOnDE',\n",
        "                         fpath=os.path.join(root,'TT_new_train_GT.zip')),\n",
        "             lbl_txt=dict(URL='https://drive.google.com/file/d/1v-pd-74EkZ3dWe6k0qppRtetjdPQ3ms1/view',\n",
        "                         fpath=os.path.join(root,'groundtruth_text.zip'))\n",
        "             )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "    print('Start download')\n",
        "    gdown.download(url=dpath['dt_img']['URL'], output=dpath['dt_img']['fpath'], quiet=True, fuzzy=True)\n",
        "\n",
        "  if not isfile(dpath['lbl_tr']['fpath']):\n",
        "    print('Start download')\n",
        "    gdown.download(url=dpath['lbl_tr']['URL'], output=dpath['lbl_tr']['fpath'], quiet=True, fuzzy=True)\n",
        "\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "  shutil.unpack_archive(dpath['lbl_tr']['fpath'],root)\n",
        "\n",
        "\n",
        "  os.rename(os.path.join(root,'Images','Test'),\n",
        "            os.path.join(root,'Images','test'))\n",
        "  \n",
        "  os.rename(os.path.join(root,'Images','Train'),\n",
        "            os.path.join(root,'Images','training'))\n",
        "  \n",
        "  os.rename(os.path.join(root,'Images'),\n",
        "            os.path.join(root,'imgs'))\n",
        "\n",
        "  move_files(os.path.join(root,'Train'),dannot)\n",
        "\n",
        "\n",
        "  from tools.data.textrecog.totaltext_converter import collect_files,collect_annotations,generate_ann\n",
        "\n",
        "\n",
        "  nproc=10\n",
        "  img_dir = osp.join(root, 'imgs')\n",
        "  gt_dir = osp.join(root, 'annotations')\n",
        "\n",
        "  set_name = {}\n",
        "  for split in ['training']:  # Originally ['training', 'test'], but since we only have `training`, we drop the `test`\n",
        "    set_name.update({split: split + '_label' + '.txt'})\n",
        "    assert osp.exists(osp.join(img_dir, split))\n",
        "\n",
        "  for split, ann_name in set_name.items():\n",
        "    print(f'Converting {split} into {ann_name}')\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert totaltext annotation'):\n",
        "      files = collect_files(osp.join(img_dir, split), osp.join(gt_dir, split))\n",
        "      image_infos = collect_annotations(files, nproc=nproc)\n",
        "      generate_ann(root, split, image_infos)\n",
        "\n",
        "  #   ├── TextOCR\n",
        "  # │   ├── image\n",
        "  # │   ├── train_label.txt\n",
        "  # │   └── val_label.txt\n",
        "totaltext('/content')"
      ],
      "metadata": {
        "id": "KCVexm9h_TgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242fb57c-f9ce-449b-9f32-71d95aee4851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#totaltext)"
      ],
      "metadata": {
        "id": "IiNwnCPcswLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def totaltext(npath,cleanup=False):\n",
        "  import gdown\n",
        "  # TODO: REQUIRE LOGIN IN CHINESE\n",
        "  root=os.path.join(npath,'totaltext')\n",
        "  dannot=os.path.join(root,'annotations','training')\n",
        "  dcrops=os.path.join(root,'imgs')\n",
        "\n",
        "\n",
        "  dpath=dict(dt_img=dict(URL = 'https://drive.google.com/open?id=1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2&authuser=0',\n",
        "                         fpath=os.path.join(root,'totaltext.zip')),\n",
        "             lbl_tr=dict(URL='https://drive.google.com/open?id=1-XrQBoU9as1PXaB_0dUrDTJgvGFFOnDE',\n",
        "                         fpath=os.path.join(root,'TT_new_train_GT.zip')),\n",
        "             lbl_txt=dict(URL='https://drive.google.com/file/d/1v-pd-74EkZ3dWe6k0qppRtetjdPQ3ms1/view',\n",
        "                         fpath=os.path.join(root,'groundtruth_text.zip'))\n",
        "             )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "    print('Start download')\n",
        "    gdown.download(url=dpath['dt_img']['URL'], output=dpath['dt_img']['fpath'], quiet=True, fuzzy=True)\n",
        "\n",
        "  if not isfile(dpath['lbl_tr']['fpath']):\n",
        "    print('Start download')\n",
        "    gdown.download(url=dpath['lbl_tr']['URL'], output=dpath['lbl_tr']['fpath'], quiet=True, fuzzy=True)\n",
        "\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "  shutil.unpack_archive(dpath['lbl_tr']['fpath'],root)\n",
        "\n",
        "\n",
        "  os.rename(os.path.join(root,'Images','Test'),\n",
        "            os.path.join(root,'Images','test'))\n",
        "  \n",
        "  os.rename(os.path.join(root,'Images','Train'),\n",
        "            os.path.join(root,'Images','training'))\n",
        "  \n",
        "  os.rename(os.path.join(root,'Images'),\n",
        "            os.path.join(root,'imgs'))\n",
        "\n",
        "  move_files(os.path.join(root,'Train'),dannot)\n",
        "\n",
        "\n",
        "  from tools.data.textdet.totaltext_converter import collect_files,collect_annotations,convert_annotations\n",
        "  import mmcv\n",
        "  import os.path as osp\n",
        "  root_path = root\n",
        "  nproc=10\n",
        "  img_dir = osp.join(root_path, 'imgs')\n",
        "  gt_dir = osp.join(root_path, 'annotations')\n",
        "\n",
        "  set_name = {}\n",
        "  for split in ['training']: # Originally ['training', 'test'], but since we only have `training`, we drop the `test`\n",
        "      set_name.update({split: 'instances_' + split + '.json'})\n",
        "      assert osp.exists(osp.join(img_dir, split))\n",
        "\n",
        "  for split, json_name in set_name.items():\n",
        "      print(f'Converting {split} into {json_name}')\n",
        "      with mmcv.Timer(\n",
        "              print_tmpl='It takes {}s to convert totaltext annotation'):\n",
        "          files = collect_files(\n",
        "              osp.join(img_dir, split), osp.join(gt_dir, split))\n",
        "          image_infos = collect_annotations(files, nproc=nproc)\n",
        "          convert_annotations(image_infos, osp.join(root_path, json_name))\n",
        "\n",
        "totaltext('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ii34OsSXn5z",
        "outputId": "d29017eb-e21c-4978-faee-b09464a00700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start download\n",
            "Start download\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeText"
      ],
      "metadata": {
        "id": "kXGhZu69-GYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#detext)"
      ],
      "metadata": {
        "id": "_5tRY7yptQ3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detext(npath,cleanup=False):\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'detext')\n",
        "  dannot_tr=os.path.join(root,'annotations','training')\n",
        "  dannot_val=os.path.join(root,'annotations','val')\n",
        "  dtrain=os.path.join(root,'imgs','training')\n",
        "  dval=os.path.join(root,'imgs','val')\n",
        "\n",
        "\n",
        "  \n",
        "  dpath=dict(dt_tr=dict(URL = 'https://rrc.cvc.uab.es/downloads/ch9_training_images.zip',\n",
        "                         fpath=os.path.join(root,'ch9_training_images.zip')),\n",
        "             dt_tr_loc=dict(URL='https://rrc.cvc.uab.es/downloads/ch9_training_localization_transcription_gt.zip',\n",
        "                         fpath=os.path.join(root,'ch9_training_localization_transcription_gt.zip')),\n",
        "             dt_val=dict(URL='https://rrc.cvc.uab.es/downloads/ch9_validation_images.zip',\n",
        "                         fpath=os.path.join(root,'ch9_validation_images.zip')),\n",
        "             dt_val_loc=dict(URL='https://rrc.cvc.uab.es/downloads/ch9_validation_localization_transcription_gt.zip',\n",
        "                         fpath=os.path.join(root,'ch9_validation_localization_transcription_gt.zip')),\n",
        "             )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dtrain,dval,dannot_tr,dannot_val]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  for dp in (['dt_tr','dt_tr_loc','dt_val','dt_val_loc']):\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],dtrain)\n",
        "  shutil.unpack_archive(dpath['dt_tr_loc']['fpath'],dannot_tr)\n",
        "  shutil.unpack_archive(dpath['dt_val']['fpath'],dval)\n",
        "  shutil.unpack_archive(dpath['dt_val_loc']['fpath'],dannot_val)\n",
        "\n",
        "  from tools.data.textrecog.detext_converter import collect_files,collect_annotations,generate_ann\n",
        "\n",
        "  import os.path as osp\n",
        "\n",
        "  # root_path=root\n",
        "  nproc=10\n",
        "  preserve_vertical=True\n",
        "  format='jsonl'\n",
        "  for split in ['training', 'val']:\n",
        "    print(f'Processing {split} set...')\n",
        "    files = collect_files(\n",
        "            osp.join(root, 'imgs', split),\n",
        "            osp.join(root, 'annotations', split))\n",
        "    image_infos = collect_annotations(files, nproc=nproc)\n",
        "    generate_ann(root, split, image_infos, preserve_vertical,format)\n",
        "\n",
        "\n",
        "  # ├── detext\n",
        "  # │   ├── crops\n",
        "  # │   ├── ignores\n",
        "  # │   ├── train_label.jsonl\n",
        "  # │   └── test_label.jsonl\n",
        "detext('/content')"
      ],
      "metadata": {
        "id": "v1j3iDEcEMYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#detext)\n",
        "\n",
        "Data preparation is similar to text recognition"
      ],
      "metadata": {
        "id": "h2mPC3tZtX57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detext(npath,cleanup=False):\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'detext')\n",
        "  dannot_tr=os.path.join(root,'annotations','training')\n",
        "  dannot_val=os.path.join(root,'annotations','val')\n",
        "  dtrain=os.path.join(root,'imgs','training')\n",
        "  dval=os.path.join(root,'imgs','val')\n",
        "\n",
        "\n",
        "  \n",
        "  dpath=dict(dt_tr=dict(URL = 'https://rrc.cvc.uab.es/downloads/ch9_training_images.zip',\n",
        "                         fpath=os.path.join(root,'ch9_training_images.zip')),\n",
        "             dt_tr_loc=dict(URL='https://rrc.cvc.uab.es/downloads/ch9_training_localization_transcription_gt.zip',\n",
        "                         fpath=os.path.join(root,'ch9_training_localization_transcription_gt.zip')),\n",
        "             dt_val=dict(URL='https://rrc.cvc.uab.es/downloads/ch9_validation_images.zip',\n",
        "                         fpath=os.path.join(root,'ch9_validation_images.zip')),\n",
        "             dt_val_loc=dict(URL='https://rrc.cvc.uab.es/downloads/ch9_validation_localization_transcription_gt.zip',\n",
        "                         fpath=os.path.join(root,'ch9_validation_localization_transcription_gt.zip')),\n",
        "             )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dtrain,dval,dannot_tr,dannot_val]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  for dp in (['dt_tr','dt_tr_loc','dt_val','dt_val_loc']):\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],dtrain)\n",
        "  shutil.unpack_archive(dpath['dt_tr_loc']['fpath'],dannot_tr)\n",
        "  shutil.unpack_archive(dpath['dt_val']['fpath'],dval)\n",
        "  shutil.unpack_archive(dpath['dt_val_loc']['fpath'],dannot_val)\n",
        "\n",
        "  # Step2: Generate instances_training.json and instances_val.json with following command:\n",
        "\n",
        "  from tools.data.textdet.detext_converter import collect_files,collect_annotations,convert_annotations\n",
        "  import mmcv\n",
        "  import os.path as osp\n",
        "  root_path = '/content/detection/detext'\n",
        "  nproc=10\n",
        "  for split in ['training', 'val']:\n",
        "      print(f'Processing {split} set...')\n",
        "      with mmcv.Timer(\n",
        "              print_tmpl='It takes {}s to convert DeText annotation'):\n",
        "          files = collect_files(\n",
        "              osp.join(root_path, 'imgs', split),\n",
        "              osp.join(root_path, 'annotations', split))\n",
        "          image_infos = collect_annotations(files, nproc=nproc)\n",
        "          convert_annotations(\n",
        "              image_infos, osp.join(root_path,\n",
        "                                    'instances_' + split + '.json'))\n",
        "\n",
        "  # After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "  # │── detext\n",
        "  # │   ├── annotations\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_test.json\n",
        "  # │   └── instances_training.json\n",
        "\n",
        "detext('/content/detection/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65mUsymeV7vj",
        "outputId": "21259259-950c-4478-9e1a-c5089dd35cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Downloading ch9_training_images.zip from https://rrc.cvc.uab.es/downloads/ch9_training_images.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading ch9_training_localization_transcription_gt.zip from https://rrc.cvc.uab.es/downloads/ch9_training_localization_transcription_gt.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading ch9_validation_images.zip from https://rrc.cvc.uab.es/downloads/ch9_validation_images.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading ch9_validation_localization_transcription_gt.zip from https://rrc.cvc.uab.es/downloads/ch9_validation_localization_transcription_gt.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm1OjUyMW5Yb",
        "outputId": "c411f872-527f-4995-fffb-fbacafc193e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training set...\n",
            "Loaded 100 images from /content/detection/detext/imgs/training\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 91.0 task/s, elapsed: 1s, ETA:     0s\n",
            "It takes 2.323080539703369s to convert DeText annotation\n",
            "Processing val set...\n",
            "Loaded 92 images from /content/detection/detext/imgs/val\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 92/92, 90.7 task/s, elapsed: 1s, ETA:     0s\n",
            "It takes 2.0592751502990723s to convert DeText annotation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAF"
      ],
      "metadata": {
        "id": "UrNFuNnIJIIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#naf)"
      ],
      "metadata": {
        "id": "hpn_0xeCtq4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GitPython"
      ],
      "metadata": {
        "id": "8-gysOEQqdTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fdcb50-6d68-4c9e-9a93-b8d97c8d0e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, GitPython\n",
            "Successfully installed GitPython-3.1.27 gitdb-4.0.9 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naf(npath,cleanup=False):\n",
        "\n",
        "  from git import Repo\n",
        "  # !pip install GitPython\n",
        "  root=os.path.join(npath,'naf')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dpath=dict(dt_tr=dict(URL = 'https://github.com/herobd/NAF_dataset/releases/download/v1.0/labeled_images.tar.gz',\n",
        "                         fpath=os.path.join(root,'labeled_images.tar.gz')))\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info ('This may take sometime to download ~800 Mb tar file')\n",
        "  check_dw(dpath['dt_tr']['fpath'],dpath['dt_tr']['URL'])\n",
        "\n",
        "  if not isfile(os.path.join(root,'NAF_dataset','train_valid_test_split.json')):\n",
        "    logging.info ('Downloading annotation from github')\n",
        "    Repo.clone_from('https://github.com/herobd/NAF_dataset.git', os.path.join(root,'NAF_dataset'))\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],root)\n",
        "  os.rename(os.path.join(root,'labeled_images'), os.path.join(root,'imgs'))\n",
        "\n",
        "  logging.info ('Reorganise file and folder')\n",
        "  shutil.move(os.path.join(root,'NAF_dataset','train_valid_test_split.json'),\n",
        "              os.path.join(root,'annotations','train_valid_test_split.json'))\n",
        "\n",
        "\n",
        "  shutil.move(os.path.join(root,'NAF_dataset','groups'),\n",
        "              os.path.join(root,'annotations','groups'))\n",
        "\n",
        "\n",
        "\n",
        "  from tools.data.textrecog.naf_converter import collect_files,collect_annotations,generate_ann\n",
        "\n",
        "  import os.path as osp\n",
        "  import mmcv\n",
        "  preserve_vertical=True\n",
        "  format='jsonl'\n",
        "  nproc=4\n",
        "  root_path = '/content/naf'\n",
        "  split_info = mmcv.load( osp.join(root_path, 'annotations', 'train_valid_test_split.json'))\n",
        "  split_info['training'] = split_info.pop('train')\n",
        "  split_info['val'] = split_info.pop('valid')\n",
        "  for split in ['training', 'val', 'test']:\n",
        "    print(f'Processing {split} set...')\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert NAF annotation'):\n",
        "      files = collect_files(osp.join(root_path, 'imgs'),\n",
        "                            osp.join(root_path, 'annotations'), split_info[split])\n",
        "      image_infos = collect_annotations(files, nproc=nproc)\n",
        "      generate_ann(root_path, split, image_infos, preserve_vertical,format)\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'NAF_dataset')) \n",
        "    os.remove(dpath['dt_tr']['fpath'])\n",
        "\n",
        "\n",
        "  #   ├── naf\n",
        "  # │   ├── crops\n",
        "  # │   ├── train_label.txt\n",
        "  # │   ├── val_label.txt\n",
        "  # │   └── test_label.txt\n",
        "\n",
        "  \n",
        "naf('/content',cleanup=True)"
      ],
      "metadata": {
        "id": "Q_M3vE32JGND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#naf)"
      ],
      "metadata": {
        "id": "jJavVohst9mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naf(npath,cleanup=False):\n",
        "\n",
        "  from git import Repo\n",
        "  # !pip install GitPython\n",
        "  root=os.path.join(npath,'naf')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dpath=dict(dt_tr=dict(URL = 'https://github.com/herobd/NAF_dataset/releases/download/v1.0/labeled_images.tar.gz',\n",
        "                         fpath=os.path.join(root,'labeled_images.tar.gz')))\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info ('This may take sometime to download ~800 Mb tar file')\n",
        "  check_dw(dpath['dt_tr']['fpath'],dpath['dt_tr']['URL'])\n",
        "\n",
        "  if not isfile(os.path.join(root,'NAF_dataset','train_valid_test_split.json')):\n",
        "    logging.info ('Downloading annotation from github')\n",
        "    Repo.clone_from('https://github.com/herobd/NAF_dataset.git', os.path.join(root,'NAF_dataset'))\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],root)\n",
        "  os.rename(os.path.join(root,'labeled_images'), os.path.join(root,'imgs'))\n",
        "\n",
        "  logging.info ('Reorganise file and folder')\n",
        "  shutil.move(os.path.join(root,'NAF_dataset','train_valid_test_split.json'),\n",
        "              os.path.join(root,'annotations','train_valid_test_split.json'))\n",
        "\n",
        "\n",
        "  shutil.move(os.path.join(root,'NAF_dataset','groups'),\n",
        "              os.path.join(root,'annotations','groups'))\n",
        "\n",
        "\n",
        "\n",
        "  # Step2: Generate instances_training.json, instances_val.json, and instances_test.json with following command:\n",
        "\n",
        "  # python tools/data/textdet/naf_converter.py PATH/TO/naf --nproc 4\n",
        "\n",
        "  # After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "  from tools.data.textdet.naf_converter import collect_files,collect_annotations,convert_annotations\n",
        "\n",
        "  import os.path as osp\n",
        "  import mmcv\n",
        "  nproc=10\n",
        "  root_path = root\n",
        "  split_info = mmcv.load(\n",
        "      osp.join(root_path, 'annotations', 'train_valid_test_split.json'))\n",
        "  split_info['training'] = split_info.pop('train')\n",
        "  split_info['val'] = split_info.pop('valid')\n",
        "  for split in ['training', 'val', 'test']:\n",
        "      print(f'Processing {split} set...')\n",
        "      with mmcv.Timer(print_tmpl='It takes {}s to convert NAF annotation'):\n",
        "          files = collect_files(\n",
        "              osp.join(root_path, 'imgs'),\n",
        "              osp.join(root_path, 'annotations'), split_info[split])\n",
        "          image_infos = collect_annotations(files, nproc=nproc)\n",
        "          convert_annotations(\n",
        "              image_infos, osp.join(root_path,\n",
        "                                    'instances_' + split + '.json'))\n",
        "        \n",
        "\n",
        "  # │── naf\n",
        "  # │   ├── annotations\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_test.json\n",
        "  # │   ├── instances_val.json\n",
        "  # │   └── instances_training.json\n",
        "\n",
        "  \n",
        "naf('/content/detection/',cleanup=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lTkxuqnZnF7",
        "outputId": "22d9105b-6db1-490a-cbed-ce9820fabb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~800 Mb tar file\n",
            "INFO:root:Downloading labeled_images.tar.gz from https://github.com/herobd/NAF_dataset/releases/download/v1.0/labeled_images.tar.gz.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading annotation from github\n",
            "INFO:root:Unpacking file\n",
            "INFO:root:Reorganise file and folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture Video DB"
      ],
      "metadata": {
        "id": "tin6DP4f3zU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#lecture-video-db)"
      ],
      "metadata": {
        "id": "oOkdlw_luWkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lv(npath,cleanup=False):\n",
        "\n",
        "  from git import Repo\n",
        "  # !pip install GitPython\n",
        "  root=os.path.join(npath,'lv')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dpath=dict(dt_tr=dict(URL = 'http://cdn.iiit.ac.in/cdn/preon.iiit.ac.in/~kartik/IIIT-CVid.zip',\n",
        "                         fpath=os.path.join(root,'IIIT-CVid.zip')))\n",
        "  \n",
        "  ch_make_folder(root)\n",
        "\n",
        "  \n",
        "\n",
        "  logging.info ('This may take sometime to download ~2.26 Gb zip file (~2 m)')\n",
        "  check_dw(dpath['dt_tr']['fpath'],dpath['dt_tr']['URL'])\n",
        "\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],root)\n",
        "\n",
        "  logging.info ('Reorganise file and folder')\n",
        "  shutil.move(os.path.join(root,'IIIT-CVid','Crops'),\n",
        "              os.path.join(root,'Crops'))\n",
        "\n",
        "  shutil.move(os.path.join(root,'IIIT-CVid','train.txt'),\n",
        "              os.path.join(root,'train_label.txt'))\n",
        "\n",
        "  shutil.move(os.path.join(root,'IIIT-CVid','val.txt'),\n",
        "              os.path.join(root,'val_label.txt'))\n",
        "  \n",
        "  shutil.move(os.path.join(root,'IIIT-CVid','test.txt'),\n",
        "              os.path.join(root,'test_label.txt'))\n",
        "\n",
        "\n",
        "  from tools.data.textrecog.lv_converter import convert_annotations\n",
        "\n",
        "  root_path=root\n",
        "  format='jsonl'\n",
        "  for split in ['train', 'val', 'test']:\n",
        "    convert_annotations(root_path, split, format)\n",
        "    print(f'{split} split converted.')\n",
        "\n",
        "  # TODO. move the test_label,train_label_val_label from iiit-cvid to root\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'NAF_dataset')) \n",
        "    os.remove(dpath['dt_tr']['fpath'])\n",
        "\n",
        "\n",
        "# ├── lv\n",
        "# │   ├── Crops\n",
        "# │   ├── train_label.jsonl\n",
        "# │   └── test_label.jsonl\n",
        "\n",
        "  \n",
        "lv('/content')"
      ],
      "metadata": {
        "id": "QViWmm6q3T7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#lecture-video-db) (Issue Generating the json file> File out of list!)"
      ],
      "metadata": {
        "id": "jAHXh960uiSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lv(npath,cleanup=False):\n",
        "\n",
        "  from git import Repo\n",
        "  # !pip install GitPython\n",
        "  root=os.path.join(npath,'lv')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dpath=dict(dt_tr=dict(URL = 'http://cdn.iiit.ac.in/cdn/preon.iiit.ac.in/~kartik/IIIT-CVid.zip',\n",
        "                         fpath=os.path.join(root,'IIIT-CVid.zip')))\n",
        "  \n",
        "  ch_make_folder(root)\n",
        "\n",
        "  \n",
        "\n",
        "  logging.info ('This may take sometime to download ~2.26 Gb zip file (download time about ~5 mins with Colab)')\n",
        "  check_dw(dpath['dt_tr']['fpath'],dpath['dt_tr']['URL'],wget_dw=True)\n",
        "\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],root)\n",
        "\n",
        "  logging.info ('Reorganise file and folder')\n",
        "  shutil.move(os.path.join(root,'IIIT-CVid','Frames'),  # Diffrence frame vs crop\n",
        "              os.path.join(root,'imgs'))\n",
        "\n",
        "  # Step2: Generate instances_training.json, instances_val.json, and instances_test.json with following command:\n",
        "\n",
        "  # python tools/data/textdet/lv_converter.py PATH/TO/lv --nproc 4\n",
        "\n",
        "  # The resulting directory structure looks like the following:\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'NAF_dataset')) \n",
        "    os.remove(dpath['dt_tr']['fpath'])\n",
        "\n",
        "\n",
        "  # │── lv\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_test.json\n",
        "  # │   ├── instances_training.json\n",
        "  # │   └── instances_val.json\n",
        "\n",
        "  \n",
        "lv('/content/detection/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N09pHCd0bMxG",
        "outputId": "24dc839a-47fa-4535-ad61-b5e504e39b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~2.26 Gb zip file (download time about ~5 mins with Colab)\n",
            "INFO:root:Unpacking file\n",
            "INFO:root:Reorganise file and folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tools.data.textdet.lv_converter import collect_files,collect_annotations,convert_annotations\n",
        "import os.path as osp\n",
        "import mmcv\n",
        "nproc=4\n",
        "root_path = '/content/detection/lv'\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f'Processing {split} set...')\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert LV annotation'):\n",
        "        files = collect_files(osp.join(root_path, 'imgs', split))\n",
        "        print(files)\n",
        "        image_infos = collect_annotations(files, nproc=nproc)\n",
        "        convert_annotations(\n",
        "            image_infos, osp.join(root_path,\n",
        "                                  'instances_' + split + '.json'))"
      ],
      "metadata": {
        "id": "VQnNfCscfzM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Step1: Download IIIT-CVid.zip to lv/.\n",
        "\n",
        "mkdir lv && cd lv\n",
        "\n",
        "# Download LV dataset\n",
        "wget http://cdn.iiit.ac.in/cdn/preon.iiit.ac.in/~kartik/IIIT-CVid.zip\n",
        "unzip -q IIIT-CVid.zip\n",
        "\n",
        "mv IIIT-CVid/Frames imgs\n",
        "\n",
        "rm IIIT-CVid.zip\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hgho9PuBupeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  LSVT"
      ],
      "metadata": {
        "id": "hjvp-Clh-Zhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#lsvt)"
      ],
      "metadata": {
        "id": "gaXN_J6RvHyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lsvt(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'lsvt')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dimg=os.path.join(root,'imgs')\n",
        "  dpath=dict(dt_img1=dict(URL = 'https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_0.tar.gz',\n",
        "                         fpath=os.path.join(root,'train_full_images_0.tar.gz')),\n",
        "             dt_img2=dict(URL = 'https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_1.tar.gz',\n",
        "                         fpath=os.path.join(root,'train_full_images_1.tar.gz')),\n",
        "             lbl=dict(URL = 'http://dataset-bj.cdn.bcebos.com/lsvt/train_full_labels.json',\n",
        "                         fpath=os.path.join(root,'train_full_labels.json')),\n",
        "            )\n",
        "  \n",
        "  \n",
        "  for dp in ([root,dimg,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  logging.info ('This may take sometime to download ~8 Gb zip file (20 m)')\n",
        "  for dp in ['dt_img1','dt_img2','lbl']:\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img1']['fpath'],root)\n",
        "  os.rename(os.path.join(root,'train_full_images_0'), os.path.join(root,'imgs'))\n",
        "\n",
        "  shutil.unpack_archive(dpath['dt_img2']['fpath'],root)\n",
        "  source_dir=os.path.join(root,'train_full_images_1')\n",
        "\n",
        "  move_files(source_dir,dimg)\n",
        "\n",
        "  logging.info ('Reorganise file and folder')\n",
        "  shutil.move(os.path.join(root,'train_full_labels.json'),dannot)\n",
        "\n",
        "\n",
        "  from tools.data.textrecog.lsvt_converter import convert_lsvt\n",
        "  root_path = root\n",
        "\n",
        "  preserve_vertical=True\n",
        "  val_ratio=0.2\n",
        "  nproc=10\n",
        "  print('Processing training set...')\n",
        "  num_train_imgs = convert_lsvt(\n",
        "      root_path=root_path,\n",
        "      split='train',\n",
        "      ratio=val_ratio,\n",
        "      preserve_vertical=preserve_vertical,\n",
        "      format=format,\n",
        "      nproc=nproc)\n",
        "  if val_ratio > 0:\n",
        "      print('Processing validation set...')\n",
        "      convert_lsvt(\n",
        "          root_path=root_path,\n",
        "          split='val',\n",
        "          ratio=val_ratio,\n",
        "          preserve_vertical=preserve_vertical,\n",
        "          format=format,\n",
        "          nproc=nproc,\n",
        "          img_start_idx=num_train_imgs)\n",
        "  print('Finish')\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'train_full_images_1')) \n",
        "    os.remove(dpath['dt_img1']['fpath'])\n",
        "    os.remove(dpath['dt_img2']['fpath'])\n",
        "\n",
        "\n",
        "# ├── lsvt\n",
        "# │   ├── crops\n",
        "# │   ├── ignores\n",
        "# │   ├── train_label.jsonl\n",
        "# │   └── val_label.jsonl (optional)\n",
        "\n",
        "  \n",
        "lsvt('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iEd-Cjd9o9b",
        "outputId": "67d1aa7c-fc7f-4490-d0a5-654dc410d6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~8 Gb zip file\n",
            "INFO:root:The file /content/lsvt/train_full_images_0.tar.gz is not availaible, downloading from https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_0.tar.gz\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:The file /content/lsvt/train_full_images_1.tar.gz is not availaible, downloading from https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_1.tar.gz\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:The file /content/lsvt/train_full_labels.json is not availaible, downloading from http://dataset-bj.cdn.bcebos.com/lsvt/train_full_labels.json\n",
            "INFO:root:Unpacking file\n",
            "INFO:root:Reorganise file and folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#lsvt)"
      ],
      "metadata": {
        "id": "6sEtRymAvRkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lsvt(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'lsvt')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dimg=os.path.join(root,'imgs')\n",
        "  dpath=dict(dt_img1=dict(URL = 'https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_0.tar.gz',\n",
        "                         fpath=os.path.join(root,'train_full_images_0.tar.gz')),\n",
        "             dt_img2=dict(URL = 'https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_1.tar.gz',\n",
        "                         fpath=os.path.join(root,'train_full_images_1.tar.gz')),\n",
        "             lbl=dict(URL = 'http://dataset-bj.cdn.bcebos.com/lsvt/train_full_labels.json',\n",
        "                         fpath=os.path.join(root,'train_full_labels.json')),\n",
        "            )\n",
        "  \n",
        "  \n",
        "  for dp in ([root,dimg,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  logging.info ('This may take sometime to download ~8 Gb zip file (20 m)')\n",
        "  for dp in ['dt_img1','dt_img2','lbl']:\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img1']['fpath'],root)\n",
        "  os.rename(os.path.join(root,'train_full_images_0'), os.path.join(root,'imgs'))\n",
        "\n",
        "  shutil.unpack_archive(dpath['dt_img2']['fpath'],root)\n",
        "  source_dir=os.path.join(root,'train_full_images_1')\n",
        "\n",
        "  move_files(source_dir,dimg)\n",
        "\n",
        "  logging.info ('Reorganise file and folder')\n",
        "  shutil.move(os.path.join(root,'train_full_labels.json'),dannot)\n",
        "\n",
        "\n",
        "\n",
        "  from tools.data.textdet.lsvt_converter import collect_lsvt_info,convert_annotations\n",
        "  import os.path as osp\n",
        "  val_ratio=0.2\n",
        "  root_path = root\n",
        "  print('Processing training set...')\n",
        "  training_infos = collect_lsvt_info(root_path, 'train', val_ratio)\n",
        "  convert_annotations(training_infos,\n",
        "                      osp.join(root_path, 'instances_training.json'))\n",
        "  if val_ratio > 0:\n",
        "      print('Processing validation set...')\n",
        "      val_infos = collect_lsvt_info(root_path, 'val', val_ratio)\n",
        "      convert_annotations(val_infos, osp.join(root_path,\n",
        "                                              'instances_val.json'))\n",
        "  print('Finish')\n",
        "\n",
        "\n",
        "  # # Annotations of LSVT test split is not publicly available, split a validation\n",
        "  # # set by adding --val-ratio 0.2\n",
        "  # python tools/data/textdet/lsvt_converter.py PATH/TO/lsvt\n",
        "\n",
        "  # After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "  # |── lsvt\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_training.json\n",
        "  # │   └── instances_val.json (optional)\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'train_full_images_1')) \n",
        "    os.remove(dpath['dt_img1']['fpath'])\n",
        "    os.remove(dpath['dt_img2']['fpath'])\n",
        "\n",
        "\n",
        "  \n",
        "lsvt('/content/detection')"
      ],
      "metadata": {
        "id": "MU3RMHHoi4qB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0d1630-c2d5-4c91-fe45-5d165533bcf1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~8 Gb zip file (20 m)\n",
            "INFO:root:Downloading train_full_images_0.tar.gz from https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_0.tar.gz.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading train_full_images_1.tar.gz from https://dataset-bj.cdn.bcebos.com/lsvt/train_full_images_1.tar.gz.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Downloading train_full_labels.json from http://dataset-bj.cdn.bcebos.com/lsvt/train_full_labels.json.\n",
            "INFO:root:Unpacking file\n",
            "INFO:root:Reorganise file and folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Yet to run!!(so far so good)\n"
      ],
      "metadata": {
        "id": "75m9Cfili1zJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3728d2-5f4d-4dc9-ed8d-399cbd57d751"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training set...\n",
            "training #24000, val #6000\n",
            "1000/24000\n",
            "2000/24000\n",
            "3000/24000\n",
            "4000/24000\n",
            "5000/24000\n",
            "6000/24000\n",
            "7000/24000\n",
            "8000/24000\n",
            "9000/24000\n",
            "10000/24000\n",
            "11000/24000\n",
            "12000/24000\n",
            "13000/24000\n",
            "14000/24000\n",
            "15000/24000\n",
            "16000/24000\n",
            "17000/24000\n",
            "18000/24000\n",
            "19000/24000\n",
            "20000/24000\n",
            "21000/24000\n",
            "22000/24000\n",
            "23000/24000\n",
            "Processing validation set...\n",
            "training #24000, val #6000\n",
            "1000/6000\n",
            "2000/6000\n",
            "3000/6000\n",
            "4000/6000\n",
            "5000/6000\n",
            "Finish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNSD"
      ],
      "metadata": {
        "id": "4jVcv8-wGrnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#funsd)"
      ],
      "metadata": {
        "id": "RV43N8yzvzR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def funsd(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'funsd')\n",
        "  dannot_ts=os.path.join(root,'annotations','test')\n",
        "  dannot_tr=os.path.join(root,'annotations','training')\n",
        "  dimg=os.path.join(root,'imgs')\n",
        "  dpath=dict(dt_img=dict(URL = 'https://guillaumejaume.github.io/FUNSD/dataset.zip',\n",
        "                         fpath=os.path.join(root,'dataset.zip'))\n",
        "            )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot_ts,dimg,dannot_tr]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  logging.info ('This may take sometime to download ~8 Gb zip file')\n",
        "  check_dw(dpath['dt_img']['fpath'],dpath['dt_img']['URL'])\n",
        "\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','training_data','images'),\n",
        "             dimg)\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','testing_data','images'),\n",
        "            dimg) # img_ts \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','testing_data','annotations'),\n",
        "             dannot_ts)\n",
        "\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','training_data','annotations'),\n",
        "             dannot_tr)\n",
        "  \n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'__MACOSX')) \n",
        "    shutil.rmtree(os.path.join(root,'dataset')) \n",
        "    os.remove(dpath['dt_img']['fpath'])\n",
        "\n",
        "  from tools.data.textrecog.funsd_converter import collect_files,collect_annotations,generate_ann\n",
        "  import os.path as osp\n",
        "  import mmcv\n",
        "  nproc=10\n",
        "  preserve_vertical=True\n",
        "  format='jsonl'\n",
        "  root_path = '/content/funsd'\n",
        "\n",
        "  for split in ['training', 'test']:\n",
        "      print(f'Processing {split} set...')\n",
        "      with mmcv.Timer(print_tmpl='It takes {}s to convert FUNSD annotation'):\n",
        "          files = collect_files(\n",
        "              osp.join(root_path, 'imgs'),\n",
        "              osp.join(root_path, 'annotations', split))\n",
        "          image_infos = collect_annotations(files, nproc=nproc)\n",
        "          generate_ann(root_path, split, image_infos, preserve_vertical,\n",
        "                      format)\n",
        "\n",
        "\n",
        "  # ├── funsd\n",
        "  # │   ├── imgs\n",
        "  # │   ├── dst_imgs\n",
        "  # │   ├── annotations\n",
        "  # │   ├── train_label.txt\n",
        "  # │   └── test_label.txt\n",
        "\n",
        "  \n",
        "funsd('/content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjoAFeM_Gtf-",
        "outputId": "fd991e91-b45d-47f2-fb1c-09761ac1bd6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~8 Gb zip file\n",
            "INFO:root:Downloading dataset.zip from https://guillaumejaume.github.io/FUNSD/dataset.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#funsd)"
      ],
      "metadata": {
        "id": "gDKz-nORv99K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def funsd(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'funsd')\n",
        "  dannot_ts=os.path.join(root,'annotations','test')\n",
        "  dannot_tr=os.path.join(root,'annotations','training')\n",
        "  dimg=os.path.join(root,'imgs')\n",
        "  dpath=dict(dt_img=dict(URL = 'https://guillaumejaume.github.io/FUNSD/dataset.zip',\n",
        "                         fpath=os.path.join(root,'dataset.zip'))\n",
        "            )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot_ts,dimg,dannot_tr]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  logging.info ('This may take sometime to download ~8 Gb zip file')\n",
        "  check_dw(dpath['dt_img']['fpath'],dpath['dt_img']['URL'],wget_dw=True)\n",
        "\n",
        "  \n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','training_data','images'),\n",
        "             dimg)\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','testing_data','images'),\n",
        "            dimg) # img_ts \n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','testing_data','annotations'),\n",
        "             dannot_ts)\n",
        "\n",
        "\n",
        "  move_files(os.path.join(root,'dataset','training_data','annotations'),\n",
        "             dannot_tr)\n",
        "  \n",
        "  from tools.data.textdet.funsd_converter import collect_files,collect_annotations,convert_annotations\n",
        "  import os.path as osp\n",
        "  import mmcv\n",
        "  nproc=10\n",
        "  root_path = root\n",
        "\n",
        "  for split in ['training', 'test']:\n",
        "      print(f'Processing {split} set...')\n",
        "      with mmcv.Timer(print_tmpl='It takes {}s to convert FUNSD annotation'):\n",
        "          files = collect_files(\n",
        "              osp.join(root_path, 'imgs'),\n",
        "              osp.join(root_path, 'annotations', split))\n",
        "          image_infos = collect_annotations(files, nproc=nproc)\n",
        "          convert_annotations(\n",
        "              image_infos, osp.join(root_path,\n",
        "                                    'instances_' + split + '.json'))\n",
        "  # The resulting directory structure looks like the following:\n",
        "\n",
        "  # │── funsd\n",
        "  # │   ├── annotations\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_test.json\n",
        "  # │   └── instances_training.json\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'__MACOSX')) \n",
        "    shutil.rmtree(os.path.join(root,'dataset')) \n",
        "    os.remove(dpath['dt_img']['fpath'])\n",
        "\n",
        "\n",
        "  \n",
        "funsd('/content/detection/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnHkpPCrN5dc",
        "outputId": "7da55b09-929b-4976-fcd3-b5662b3ff98c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~8 Gb zip file\n",
            "INFO:root:Downloading dataset.zip from https://guillaumejaume.github.io/FUNSD/dataset.zip.\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COCO Text v2"
      ],
      "metadata": {
        "id": "hQncXZGGO5FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#coco-text-v2)"
      ],
      "metadata": {
        "id": "eDEQgp3JwRCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coco_textv2(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'coco_textv2')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "\n",
        "  # dimg=os.path.join(root,'imgs')\n",
        "\n",
        "  dpath=dict(dt_tr=dict(URL = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                         fpath=os.path.join(root,'train2014.zip')),\n",
        "             dt_trx=dict(URL = 'https://github.com/bgshih/cocotext/releases/download/dl/cocotext.v2.zip',\n",
        "                         fpath=os.path.join(root,'cocotext.v2.zip'))\n",
        "            )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  # Potential error with Google Colab wherby the session crashed after using all available RAM\n",
        "  logging.info ('This may take sometime to download ~12.58 Gb zip file')\n",
        "\n",
        "  for dp in ['dt_tr','dt_trx']:\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'],wget=False)\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],root)\n",
        "  shutil.unpack_archive(dpath['dt_trx']['fpath'],root)\n",
        "\n",
        "  os.rename(os.path.join(root,'train2014'),\n",
        "            os.path.join(root,'imgs'))\n",
        "\n",
        "  shutil.move(os.path.join(root, 'cocotext.v2.json'),\n",
        "              dannot)\n",
        "\n",
        "\n",
        "  from tools.data.textrecog.cocotext_converter import convert_cocotext\n",
        "  root_path = root\n",
        "  preserve_vertical=True\n",
        "  nproc=10\n",
        "  format='jsonl'\n",
        "  print('Processing training set...')\n",
        "  num_train_imgs = convert_cocotext(\n",
        "      root_path=root_path,\n",
        "      split='train',\n",
        "      preserve_vertical=preserve_vertical,\n",
        "      format=format,\n",
        "      nproc=nproc)\n",
        "  print('Processing validation set...')\n",
        "  convert_cocotext(\n",
        "      root_path=root_path,\n",
        "      split='val',\n",
        "      preserve_vertical=preserve_vertical,\n",
        "      format=format,\n",
        "      nproc=nproc,\n",
        "      img_start_idx=num_train_imgs)\n",
        "  print('Finish')\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'dataset')) \n",
        "    os.remove(dpath['dt_tr']['fpath'])\n",
        "    os.remove(dpath['dt_trx']['fpath'])\n",
        "\n",
        "  #   ├── coco_textv2\n",
        "  # │   ├── crops\n",
        "  # │   ├── ignores\n",
        "  # │   ├── train_label.jsonl\n",
        "  # │   └── val_label.jsonl\n",
        "    \n",
        "coco_textv2('/content')"
      ],
      "metadata": {
        "id": "EsdE7-AAO-sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c101a17f-25d0-42d5-c477-5d94ea330a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~12.58 Gb zip file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#coco-text-v2)"
      ],
      "metadata": {
        "id": "s8xpWK6Lwd2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coco_textv2(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'coco_textv2')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "\n",
        "  # dimg=os.path.join(root,'imgs')\n",
        "\n",
        "  dpath=dict(dt_tr=dict(URL = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                         fpath=os.path.join(root,'train2014.zip')),\n",
        "             dt_trx=dict(URL = 'https://github.com/bgshih/cocotext/releases/download/dl/cocotext.v2.zip',\n",
        "                         fpath=os.path.join(root,'cocotext.v2.zip'))\n",
        "            )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  # Potential error with Google Colab wherby the session crashed after using all available RAM\n",
        "  logging.info ('This may take sometime to download ~12.58 Gb zip file')\n",
        "\n",
        "  for dp in ['dt_tr','dt_trx']:\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'],wget_dw=True)\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_tr']['fpath'],root)\n",
        "  shutil.unpack_archive(dpath['dt_trx']['fpath'],root)\n",
        "\n",
        "  os.rename(os.path.join(root,'train2014'),\n",
        "            os.path.join(root,'imgs'))\n",
        "\n",
        "  shutil.move(os.path.join(root, 'cocotext.v2.json'),\n",
        "              dannot)\n",
        "\n",
        "\n",
        "\n",
        "  from tools.data.textdet.cocotext_converter import collect_cocotext_info,convert_annotations\n",
        "  import os.path as osp\n",
        "\n",
        "  # preserve_vertical=True\n",
        "  # nproc=10\n",
        "  # format='jsonl'\n",
        "\n",
        "\n",
        "  root_path =root\n",
        "  print('Processing training set...')\n",
        "  training_infos = collect_cocotext_info(root_path, 'train')\n",
        "  convert_annotations(training_infos,\n",
        "                      osp.join(root_path, 'instances_training.json'))\n",
        "  print('Processing validation set...')\n",
        "  val_infos = collect_cocotext_info(root_path, 'val')\n",
        "  convert_annotations(val_infos, osp.join(root_path, 'instances_val.json'))\n",
        "  print('Finish')\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'dataset')) \n",
        "    os.remove(dpath['dt_tr']['fpath'])\n",
        "    os.remove(dpath['dt_trx']['fpath'])\n",
        "\n",
        "\n",
        "    \n",
        "coco_textv2('/content')"
      ],
      "metadata": {
        "id": "4VwEFCDejqOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3021f8-0dc9-4b0f-ce53-8e24e937b5df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:This may take sometime to download ~12.58 Gb zip file\n",
            "INFO:root:Downloading train2014.zip from http://images.cocodataset.org/zips/train2014.zip.\n",
            "INFO:root:Downloading cocotext.v2.zip from https://github.com/bgshih/cocotext/releases/download/dl/cocotext.v2.zip.\n",
            "INFO:root:Unpacking file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Step1: Download image train2014.zip and annotation cocotext.v2.zip to coco_textv2/.\n",
        "\n",
        "mkdir coco_textv2 && cd coco_textv2\n",
        "mkdir annotations\n",
        "\n",
        "# Download COCO Text v2 dataset\n",
        "wget http://images.cocodataset.org/zips/train2014.zip\n",
        "wget https://github.com/bgshih/cocotext/releases/download/dl/cocotext.v2.zip\n",
        "unzip -q train2014.zip && unzip -q cocotext.v2.zip\n",
        "\n",
        "mv train2014 imgs && mv cocotext.v2.json annotations/\n",
        "\n",
        "rm train2014.zip && rm -rf cocotext.v2.zip\n",
        "\n",
        "Step2: Generate instances_training.json and instances_val.json with the following command:\n",
        "\n",
        "python tools/data/textdet/cocotext_converter.py PATH/TO/coco_textv2\n",
        "\n",
        "After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "│── coco_textv2\n",
        "│   ├── annotations\n",
        "│   ├── imgs\n",
        "│   ├── instances_training.json\n",
        "│   └── instances_val.json\n"
      ],
      "metadata": {
        "id": "H0mSarEYwt3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vintext"
      ],
      "metadata": {
        "id": "U0_PTFvCTPcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#vintext)"
      ],
      "metadata": {
        "id": "cPsCXwP2w-3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml\" -O vintext.zip && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "e4amqqEoXhkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vintext(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'vintext')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "\n",
        "  dimg=os.path.join(root,'imgs')\n",
        "  dimg_tr=os.path.join(dimg,'training')\n",
        "  dimg_ts=os.path.join(dimg,'test')\n",
        "  dimg_ur=os.path.join(dimg,'unseen_test')\n",
        "  dpath=dict(dt_img=dict(URL = 'https://docs.google.com/uc?export=download&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml',\n",
        "                         fpath=os.path.join(root,'vintext.zip'))\n",
        "            )\n",
        "  \n",
        "  for dp in [root,dannot,dimg,\n",
        "             dimg_tr,\n",
        "             dimg_ur,\n",
        "             dimg_ts]:\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  if not isfile(dpath['dt_img']['fpath']):\n",
        "      fd,fname=os.path.split(dpath['dt_img']['fpath'])\n",
        "      logging.info (f'Please download the {fname} and stored under {fd}.or use the command')\n",
        "      # raise ('wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml\" -O vintext.zip && rm -rf /tmp/cookies.txt')\n",
        "\n",
        "\n",
        "  logging.info(f'Unpacking file')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "\n",
        "  logging.info(f'Moving files')\n",
        "  for dfrom,dto in zip(['labels','train_images','test_image','unseen_test_images'],\n",
        "                       [dannot,dimg_tr,dimg_ts,dimg_ur]):\n",
        "    move_files(os.path.join(root,'vietnamese',dfrom),dto)\n",
        "\n",
        "\n",
        "  import os.path as osp\n",
        "  import mmcv\n",
        "  from tools.data.textrecog.vintext_converter import collect_files,collect_annotations,generate_ann\n",
        "  root_path = root\n",
        "  for split in ['training', 'test', 'unseen_test']:\n",
        "      print(f'Processing {split} set...')\n",
        "      with mmcv.Timer(\n",
        "              print_tmpl='It takes {}s to convert VinText annotation'):\n",
        "          files = collect_files(\n",
        "              osp.join(root_path, 'imgs', split),\n",
        "              osp.join(root_path, 'annotations'))\n",
        "          image_infos = collect_annotations(files, nproc=nproc)\n",
        "          generate_ann(root_path, split, image_infos, preserve_vertical,\n",
        "                      format)\n",
        "        \n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    shutil.rmtree(os.path.join(root,'vietnamese')) \n",
        "    os.remove(dpath['dt_img']['fpath'])\n",
        "\n",
        "\n",
        "# ├── vintext\n",
        "# │   ├── crops\n",
        "# │   ├── ignores\n",
        "# │   ├── train_label.jsonl\n",
        "# │   ├── test_label.jsonl\n",
        "# │   └── unseen_test_label.jsonl\n",
        "    \n",
        "vintext('/content')"
      ],
      "metadata": {
        "id": "5613fs7IbXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml\" -O vintext.zip && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "hTLECPUKeItV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#vintext)"
      ],
      "metadata": {
        "id": "-S_0FtuJimcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Step1: Download vintext.zip to vintext\n",
        "\n",
        "mkdir vintext && cd vintext\n",
        "\n",
        "# Download dataset from google drive\n",
        "wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml' -O- │ sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml\" -O vintext.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Extract images and annotations\n",
        "unzip -q vintext.zip && rm vintext.zip\n",
        "mv vietnamese/labels ./ && mv vietnamese/test_image ./ && mv vietnamese/train_images ./ && mv vietnamese/unseen_test_images ./\n",
        "rm -rf vietnamese\n",
        "\n",
        "# Rename files\n",
        "mv labels annotations && mv test_image test && mv train_images  training && mv unseen_test_images  unseen_test\n",
        "mkdir imgs\n",
        "mv training imgs/ && mv test imgs/ && mv unseen_test imgs/\n",
        "\n",
        "Step2: Generate instances_training.json, instances_test.json and instances_unseen_test.json\n",
        "\n",
        "python tools/data/textdet/vintext_converter.py PATH/TO/vintext --nproc 4\n",
        "\n",
        "After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "│── vintext\n",
        "│   ├── annotations\n",
        "│   ├── imgs\n",
        "│   ├── instances_test.json\n",
        "│   ├── instances_unseen_test.json\n",
        "│   └── instances_training.json\n"
      ],
      "metadata": {
        "id": "1AH903E9QiMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BID"
      ],
      "metadata": {
        "id": "0CIw6W0UTjdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#bid)"
      ],
      "metadata": {
        "id": "C8lNFu3TxSxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "rR2GIN-hi0Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bid(npath,cleanup=False):\n",
        "\n",
        "    import gdown\n",
        "\n",
        "    from glob import glob\n",
        "\n",
        "    root=os.path.join(npath,'bid')\n",
        "\n",
        "\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "\n",
        "    dimg=os.path.join(root,'imgs')\n",
        "\n",
        "    dpath=dict(dt_img=dict(URL = 'https://drive.google.com/uc?id=1Oi88TRcpdjZmJ79WDLb9qFlBNG8q2De6&export=download',\n",
        "                           fpath=os.path.join(root,'BID Dataset.zip'))\n",
        "               )\n",
        "\n",
        "\n",
        "    for dp in ([root,dannot,dimg]):\n",
        "        ch_make_folder(dp)\n",
        "\n",
        "\n",
        "    if not isfile(dpath['dt_img']['fpath']):\n",
        "        logging.info ('It may take sometime to download 6.81 Gb zip file from Google Drive')\n",
        "        # url = 'https://drive.google.com/uc?id=1Oi88TRcpdjZmJ79WDLb9qFlBNG8q2De6&export=download'\n",
        "        gdown.download(dpath['dt_img']['URL'], dpath['dt_img']['fpath'], quiet=False)\n",
        "\n",
        "    logging.info ('It may take sometime to extract 6.81 Gb zip')\n",
        "    # shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "    ls_jpg=glob(os.path.join(root,'BID Dataset','*','*.jpg'))\n",
        "    ls_txt=glob(os.path.join(root,'BID Dataset','*','*.txt'))\n",
        "\n",
        "    for ls_ext,ls_dest in zip([ls_jpg,ls_txt],\n",
        "                      [dimg,dannot]):\n",
        "        move_files_to_des(ls_ext,ls_dest)\n",
        "\n",
        "\n",
        "    root_path = root\n",
        "    preserve_vertical=True\n",
        "    nproc=10\n",
        "    format='jsonl'\n",
        "    val_ratio=0.2\n",
        "    import os.path as osp\n",
        "    import mmcv\n",
        "    from tools.data.textrecog.bid_converter import collect_files,collect_annotations,generate_ann\n",
        "    # root_path = '/content/bid'\n",
        "    with mmcv.Timer(print_tmpl='It takes {}s to convert BID annotation'):\n",
        "        files = collect_files(\n",
        "            osp.join(root_path, 'imgs'), osp.join(root_path, 'annotations'))\n",
        "        print('Start Collect annotation')\n",
        "        image_infos = collect_annotations(files, nproc=nproc)\n",
        "        print('Start Generating  annotation')\n",
        "        generate_ann(root_path, image_infos, preserve_vertical,\n",
        "                     val_ratio, format)\n",
        "        \n",
        "                \n",
        "\n",
        "\n",
        "        \n",
        "    if cleanup:\n",
        "        logging.info ('Cleaning up')\n",
        "        move_files(os.path.join(root,'vietnamese','labels'),dannot)\n",
        "  \n",
        "#     ├── BID\n",
        "# │   ├── crops\n",
        "# │   ├── ignores\n",
        "# │   ├── train_label.jsonl\n",
        "# │   └── val_label.jsonl (optional)\n",
        "bid('/content')"
      ],
      "metadata": {
        "id": "QJqTXZieiJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#bid)"
      ],
      "metadata": {
        "id": "-52LNmG-xbQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bid(npath,cleanup=False):\n",
        "\n",
        "    import gdown\n",
        "\n",
        "    from glob import glob\n",
        "\n",
        "    root=os.path.join(npath,'bid')\n",
        "\n",
        "\n",
        "    dannot=os.path.join(root,'annotations')\n",
        "\n",
        "    dimg=os.path.join(root,'imgs')\n",
        "\n",
        "    dpath=dict(dt_img=dict(URL = 'https://drive.google.com/uc?id=1Oi88TRcpdjZmJ79WDLb9qFlBNG8q2De6&export=download',\n",
        "                           fpath=os.path.join(root,'BID Dataset.zip'))\n",
        "               )\n",
        "\n",
        "\n",
        "    for dp in ([root,dannot,dimg]):\n",
        "        ch_make_folder(dp)\n",
        "\n",
        "\n",
        "    if not isfile(dpath['dt_img']['fpath']):\n",
        "        logging.info ('It may take sometime to download 6.81 Gb zip file from Google Drive')\n",
        "        # url = 'https://drive.google.com/uc?id=1Oi88TRcpdjZmJ79WDLb9qFlBNG8q2De6&export=download'\n",
        "        gdown.download(dpath['dt_img']['URL'], dpath['dt_img']['fpath'], quiet=False)\n",
        "\n",
        "    logging.info ('It may take sometime to extract 6.81 Gb zip')\n",
        "    shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "\n",
        "    ls_jpg=glob(os.path.join(root,'BID Dataset','*','*.jpg'))\n",
        "    ls_txt=glob(os.path.join(root,'BID Dataset','*','*.txt'))\n",
        "\n",
        "    for ls_ext,ls_dest in zip([ls_jpg,ls_txt],\n",
        "                      [dimg,dannot]):\n",
        "        move_files_to_des(ls_ext,ls_dest)\n",
        "\n",
        "\n",
        "\n",
        "  import os.path as osp\n",
        "  import mmcv\n",
        "  from tools.data.textdet.bid_converter import collect_files,collect_annotations,split_train_val_list,convert_annotations\n",
        "\n",
        "  preserve_vertical=True\n",
        "  nproc=10\n",
        "  format='jsonl'\n",
        "  val_ratio=0.2\n",
        "  root_path =root\n",
        "  with mmcv.Timer(print_tmpl='It takes {}s to convert BID annotation'):\n",
        "      files = collect_files(\n",
        "          osp.join(root_path, 'imgs'), osp.join(root_path, 'annotations'))\n",
        "      image_infos = collect_annotations(files, nproc=nproc)\n",
        "      if val_ratio:\n",
        "          image_infos = split_train_val_list(image_infos, val_ratio)\n",
        "          splits = ['training', 'val']\n",
        "      else:\n",
        "          image_infos = [image_infos]\n",
        "          splits = ['training']\n",
        "      for i, split in enumerate(splits):\n",
        "          convert_annotations(\n",
        "              image_infos[i],\n",
        "              osp.join(root_path, 'instances_' + split + '.json'))\n",
        "          \n",
        "  # Step3: - Step3: Generate instances_training.json and instances_val.json (optional). Since the original dataset doesn’t have a validation set, you may specify --val-ratio to split the dataset. E.g., if val-ratio is 0.2, then 20% of the data are left out as the validation set in this example.\n",
        "\n",
        "  # python tools/data/textdet/bid_converter.py PATH/TO/BID --nproc 4\n",
        "\n",
        "  # After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "  # │── BID\n",
        "  # │   ├── annotations\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_training.json\n",
        "  # │   └── instances_val.json (optional)\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "        \n",
        "    if cleanup:\n",
        "        logging.info ('Cleaning up')\n",
        "        move_files(os.path.join(root,'vietnamese','labels'),dannot)\n",
        "\n",
        "bid('/content/detection')"
      ],
      "metadata": {
        "id": "KMFRdpWAkFQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf62988-b22a-437d-9a46-4dc724580cd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:It may take sometime to extract 6.81 Gb zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Yet to run\n",
        "\n"
      ],
      "metadata": {
        "id": "hFEFIFI7kJ0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a5a71b-fbec-458c-dd3e-0bb42f9915e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 57600 images from /content/detection/bid/imgs\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>] 57600/57600, 92.0 task/s, elapsed: 626s, ETA:     0s\n",
            "It takes 699.8836359977722s to convert BID annotation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Step1: Download BID Dataset.zip\n",
        "\n",
        "Step2: Run the following commands to preprocess the dataset\n",
        "\n",
        "# Rename\n",
        "mv BID\\ Dataset.zip BID_Dataset.zip\n",
        "\n",
        "# Unzip and Rename\n",
        "unzip -q BID_Dataset.zip && rm BID_Dataset.zip\n",
        "mv BID\\ Dataset BID\n",
        "\n",
        "# The BID dataset has a problem of permission, and you may\n",
        "# add permission for this file\n",
        "chmod -R 777 BID\n",
        "cd BID\n",
        "mkdir imgs && mkdir annotations\n",
        "\n",
        "# For images and annotations\n",
        "mv CNH_Aberta/*in.jpg imgs && mv CNH_Aberta/*txt annotations && rm -rf CNH_Aberta\n",
        "mv CNH_Frente/*in.jpg imgs && mv CNH_Frente/*txt annotations && rm -rf CNH_Frente\n",
        "mv CNH_Verso/*in.jpg imgs && mv CNH_Verso/*txt annotations && rm -rf CNH_Verso\n",
        "mv CPF_Frente/*in.jpg imgs && mv CPF_Frente/*txt annotations && rm -rf CPF_Frente\n",
        "mv CPF_Verso/*in.jpg imgs && mv CPF_Verso/*txt annotations && rm -rf CPF_Verso\n",
        "mv RG_Aberto/*in.jpg imgs && mv RG_Aberto/*txt annotations && rm -rf RG_Aberto\n",
        "mv RG_Frente/*in.jpg imgs && mv RG_Frente/*txt annotations && rm -rf RG_Frente\n",
        "mv RG_Verso/*in.jpg imgs && mv RG_Verso/*txt annotations && rm -rf RG_Verso\n",
        "\n",
        "# Remove unnecessary files\n",
        "rm -rf desktop.ini\n",
        "\n"
      ],
      "metadata": {
        "id": "j3YKWs_4xlzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ArT"
      ],
      "metadata": {
        "id": "x-YGMjV3UC4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Recognition](https://mmocr.readthedocs.io/en/latest/datasets/recog.html#art)"
      ],
      "metadata": {
        "id": "0ZuF2BDLQuDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def art(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'art')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "\n",
        "  dcrop=os.path.join(root,'crops')\n",
        "\n",
        "  dpath=dict(dt_img=dict(URL = 'https://dataset-bj.cdn.bcebos.com/art/train_task2_images.tar.gz',\n",
        "                         fpath=os.path.join(root,'train_task2_images.tar.gz')),\n",
        "             lbl=dict(URL = 'https://dataset-bj.cdn.bcebos.com/art/train_task2_labels.json',\n",
        "                         fpath=os.path.join(root,'train_task2_labels.json'))\n",
        "            )\n",
        "  for dp in ([root,dannot,dcrop]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info ('It may take sometime to extract 439 Mb tar.gz')\n",
        "\n",
        "  for dp in (['dt_img','lbl']):\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "  logging.info ('It may take sometime to extract 6.81 Gb zip')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "  os.rename(os.path.join(root,'train_task2_images'), os.path.join(root,'imgs'))\n",
        "\n",
        "  \n",
        "  shutil.move(os.path.join(root,'train_task2_labels.json'),\n",
        "              os.path.join(dannot,'train_task2_labels.json'))\n",
        "  \n",
        "\n",
        "  root_path = root\n",
        "  preserve_vertical=True\n",
        "  nproc=10\n",
        "  format='jsonl'\n",
        "  val_ratio=0.2\n",
        "\n",
        "  from tools.data.textrecog.art_converter import convert_art\n",
        "\n",
        "\n",
        "  print('Processing training set...')\n",
        "  convert_art(\n",
        "      root_path=root_path,\n",
        "      split='train',\n",
        "      ratio=val_ratio,\n",
        "      format=format)\n",
        "  if val_ratio > 0:\n",
        "      print('Processing validation set...')\n",
        "      convert_art(\n",
        "          root_path=root_path,\n",
        "          split='val',\n",
        "          ratio=val_ratio,\n",
        "          format=format)\n",
        "  print('Finish')\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    os.remove(dpath['dt_img']['fpath'])\n",
        "# │── art\n",
        "# │   ├── crops\n",
        "# │   ├── train_label.jsonl\n",
        "# │   └── val_label.jsonl (optional)\n",
        "art('/content')"
      ],
      "metadata": {
        "id": "8zmq86HVSSJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#art)"
      ],
      "metadata": {
        "id": "Sj5KWAldQ3Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def art(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'art')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dpath=dict(dt_img=dict(URL = 'https://dataset-bj.cdn.bcebos.com/art/train_images.tar.gz',\n",
        "                         fpath=os.path.join(root,'train_images.tar.gz')),\n",
        "             lbl=dict(URL = 'https://dataset-bj.cdn.bcebos.com/art/train_labels.json',\n",
        "                         fpath=os.path.join(root,'train_labels.json'))\n",
        "            )\n",
        "  for dp in ([root,dannot,dcrop]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info ('It may take sometime to extract 439 Mb tar.gz')\n",
        "\n",
        "  for dp in (['dt_img','lbl']):\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "  logging.info ('It may take sometime to extract 6.81 Gb zip')\n",
        "  shutil.unpack_archive(dpath['dt_img']['fpath'],root)\n",
        "  os.rename(os.path.join(root,'train_images'), os.path.join(root,'imgs')) # The diffrent between recog and detection\n",
        "\n",
        "  shutil.move(dpath['lbl']['fpath'],dannot)\n",
        "  \n",
        "\n",
        "  import os.path as osp\n",
        "\n",
        "  from tools.data.textdet.art_converter import collect_art_info, convert_annotations\n",
        "\n",
        "  root_path = root\n",
        "  val_ratio=0.2\n",
        "  print('Processing training set...')\n",
        "  training_infos = collect_art_info(root_path, 'train', val_ratio)\n",
        "  convert_annotations(training_infos,\n",
        "                      osp.join(root_path, 'instances_training.json'))\n",
        "  if val_ratio > 0:\n",
        "      print('Processing validation set...')\n",
        "      val_infos = collect_art_info(root_path, 'val', val_ratio)\n",
        "      convert_annotations(val_infos, osp.join(root_path,\n",
        "                                              'instances_val.json'))\n",
        "  print('Finish')\n",
        "\n",
        "\n",
        "  # Annotations of ArT test split is not publicly available, split a validation set by adding --val-ratio 0.2\n",
        "  # python tools/data/textdet/art_converter.py PATH/TO/art --nproc 4\n",
        "\n",
        "  # After running the above codes, the directory structure should be as follows:\n",
        "\n",
        "  # │── art\n",
        "  # │   ├── annotations\n",
        "  # │   ├── imgs\n",
        "  # │   ├── instances_training.json\n",
        "  # │   └── instances_val.json (optional)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # if cleanup:\n",
        "  #   logging.info ('Cleaning up')\n",
        "  #   os.remove(dpath['dt_img']['fpath'])\n",
        "\n",
        "art('/content/detection/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FqtJ2wGXL9G",
        "outputId": "b9a6c24f-df4a-48c1-d34b-c00946410ab0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:It may take sometime to extract 439 Mb tar.gz\n",
            "INFO:root:It may take sometime to extract 6.81 Gb zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he1msMAxZ5Vk",
        "outputId": "6e5cbaf2-59b6-48e2-ee6b-74509d295db0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training set...\n",
            "training #4482, val #1121\n",
            "1000/4482\n",
            "2000/4482\n",
            "3000/4482\n",
            "4000/4482\n",
            "Processing validation set...\n",
            "training #4482, val #1121\n",
            "1000/1121\n",
            "Finish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CTW1500\n"
      ],
      "metadata": {
        "id": "M3UvJEVTYKHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#ctw1500)"
      ],
      "metadata": {
        "id": "W1ONjA-vRIxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ctw1500(npath,cleanup=False):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'ctw1500')\n",
        "  dtr=os.path.join(root,'training')\n",
        "  dts=os.path.join(root,'test')\n",
        "\n",
        "  dpath=dict(lbl_tr=dict(URL = 'https://universityofadelaide.box.com/shared/static/jikuazluzyj4lq6umzei7m2ppmt3afyw.zip',\n",
        "                         fpath=os.path.join(root,'train_labels.zip')),\n",
        "             lbl_ts=dict(URL = 'https://cloudstor.aarnet.edu.au/plus/s/uoeFl0pCN9BOCN5/download',\n",
        "                         fpath=os.path.join(root,'test_labels.zip')),\n",
        "             dt_tr=dict(URL = 'https://universityofadelaide.box.com/shared/static/py5uwlfyyytbb2pxzq9czvu6fuqbjdh8.zip',\n",
        "                         fpath=os.path.join(root,'train_images.zip')),\n",
        "             dt_ts=dict(URL = 'https://universityofadelaide.box.com/shared/static/t4w48ofnqkdw7jyc4t11nsukoeqk9c3d.zip',\n",
        "                         fpath=os.path.join(root,'test_images.zip'))\n",
        "            )\n",
        "  for dp in ([root,dtr,dts]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "\n",
        "  logging.info ('It may take sometime to extract 439 Mb tar.gz')\n",
        "  for dp in (['lbl_tr','lbl_ts','dt_tr','dt_ts']):\n",
        "    check_dw(dpath[dp]['fpath'],dpath[dp]['URL'])\n",
        "\n",
        "\n",
        "  logging.info ('It may take sometime to extract 6.81 Gb zip')\n",
        "  for dp,fdir in zip(['lbl_tr','lbl_ts','dt_tr','dt_ts'],\n",
        "                [dtr,dts,dtr,dts]):\n",
        "    shutil.unpack_archive(dpath[dp]['fpath'],fdir)\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    for dp in (['lbl_tr','lbl_ts','dt_tr','dt_ts']):\n",
        "      print(dpath[dp]['fpath'])\n",
        "      os.remove(dpath[dp]['fpath'])\n",
        "\n",
        "# ├── ctw1500\n",
        "# │   ├── imgs\n",
        "# │   ├── annotations\n",
        "# │   ├── instances_training.json\n",
        "# │   └── instances_val.json\n",
        "ctw1500('/content')"
      ],
      "metadata": {
        "id": "uyx1wcQ_aeRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CurvedSynText150k (KIV)\n",
        "\n",
        "Since the file is to large, about 32 Gb, adviseable to first download and store in the folder"
      ],
      "metadata": {
        "id": "bGpgfGZoYalc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#curvedsyntext150k)"
      ],
      "metadata": {
        "id": "79TStxz5n7f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def curvedsyntext(npath,cleanup=False):\n",
        "\n",
        "\n",
        "  Warning.warn('WIP: Still find an alternative on automatically download from Google Drive')\n",
        "  root=os.path.join(npath,'curvedsyntext')\n",
        "  dtr=os.path.join(root,'training')\n",
        "  dts=os.path.join(root,'test')\n",
        "\n",
        "  dpath=dict(dt_1=dict(URL = 'https://drive.google.com/open?id=1OSJ-zId2h3t_-I7g_wUkrK-VqQy153Kj&authuser=0',\n",
        "                         fpath=os.path.join(root,'syntext1.zip ')),\n",
        "             dt_2=dict(URL = 'https://drive.google.com/open?id=1EzkcOlIgEp5wmEubvHb7-J5EImHExYgY&authuser=0',\n",
        "                         fpath=os.path.join(root,'syntext2.zip ')),\n",
        "            )\n",
        "  for dp in ([root,dtr,dts]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  import gdown\n",
        "  for dp in (['dt_1','dt_2']):\n",
        "    logging.info ('It may take sometime to download 32 Gb zip file from Google Drive')\n",
        "    if not isfile(dpath[dp]['fpath']):\n",
        "        gdown.download(dpath[dp]['URL'], dpath[dp]['fpath'], quiet=False)\n",
        "\n",
        "\n",
        "\n",
        "  logging.info ('It may take sometime to extract 6.81 Gb zip')\n",
        "  for dp,fdir in zip(['dt_1','dt_2'],\n",
        "                [dtr,dts]):\n",
        "    shutil.unpack_archive(dpath[dp]['fpath'],fdir)\n",
        "\n",
        "\n",
        "  if cleanup:\n",
        "    logging.info ('Cleaning up')\n",
        "    for dp in (['lbl_tr','lbl_ts','dt_tr','dt_ts']):\n",
        "      print(dpath[dp]['fpath'])\n",
        "      os.remove(dpath[dp]['fpath'])\n",
        "\n",
        "# ├── CurvedSynText150k\n",
        "# │   ├── syntext_word_eng\n",
        "# │   ├── emcs_imgs\n",
        "# │   └── instances_training.json\n",
        "curvedsyntext('/content')"
      ],
      "metadata": {
        "id": "pu5B07TMYowm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SROIE\n",
        "\n",
        "warning.warn('WIP since need to download from Google Drive')\n",
        "\n",
        "Links\n",
        "\n",
        "Main page\n",
        "https://rrc.cvc.uab.es/?ch=13&com=downloads\n",
        "\n",
        "\n",
        "https://rrc.cvc.uab.es/?com=downloads&action=download&ch=13&f=aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL29wZW4/aWQ9MVNoSXROV1h5aVkxdEZETTVXMDJiY2VIdUpqeWVlSmwy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://rrc.cvc.uab.es/?com=downloads&action=download&ch=13&f=aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL29wZW4/aWQ9MVNoSXROV1h5aVkxdEZETTVXMDJiY2VIdUpqeWVlSmwy\n",
        "\n",
        "\n",
        "https://rrc.cvc.uab.es/?com=downloads&action=download&ch=13&f=aHR0cHM6Ly9ycmMuY3ZjLnVhYi5lcy9kb3dubG9hZHMvU1JPSUVfdGVzdF9pbWFnZXNfdGFza18zLnppcA==\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ei_5LsIGYh_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Text Detection](https://mmocr.readthedocs.io/en/latest/datasets/det.html#sroie)"
      ],
      "metadata": {
        "id": "iC2o79BBRhWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warning.warn('WIP since need to download from Google Drive')\n",
        "\n",
        "Links\n",
        "\n",
        "Main page\n",
        "https://rrc.cvc.uab.es/?ch=13&com=downloads\n",
        "\n",
        "\n",
        "https://rrc.cvc.uab.es/?com=downloads&action=download&ch=13&f=aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL29wZW4/aWQ9MVNoSXROV1h5aVkxdEZETTVXMDJiY2VIdUpqeWVlSmwy"
      ],
      "metadata": {
        "id": "xMBi9xaVpvfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KAIST (KOREAN) NA"
      ],
      "metadata": {
        "id": "nDvLKtYIYnm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MTWI (Chinese) NA"
      ],
      "metadata": {
        "id": "iwcu4rydYpvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReCTS\n",
        "\n",
        "obust Reading Challenge on Reading Chinese Text on Signboard"
      ],
      "metadata": {
        "id": "7kTJOcBKYtEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IIIT-ILST\n",
        "\n",
        "\n",
        "Devanagari ,Malayalam, Telugu"
      ],
      "metadata": {
        "id": "HKb9oKjvYvjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://iiitaphyd-my.sharepoint.com/personal/minesh_mathew_research_iiit_ac_in/_layouts/15/download.aspx?UniqueId=dffd4198%2Dbcdc%2D4994%2D990d%2D682525da47dd"
      ],
      "metadata": {
        "id": "SCVeHzCmrbMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://iiitaphyd-my.sharepoint.com/personal/minesh_mathew_research_iiit_ac_in/_layouts/15/download.aspx?UniqueId=dffd4198%2Dbcdc%2D4994%2D990d%2D682525da47dd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRwh7vjMrXMK",
        "outputId": "ebab45f5-0104-4d08-f45a-8293115db3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 06:50:28--  https://iiitaphyd-my.sharepoint.com/personal/minesh_mathew_research_iiit_ac_in/_layouts/15/download.aspx?UniqueId=dffd4198%2Dbcdc%2D4994%2D990d%2D682525da47dd\n",
            "Resolving iiitaphyd-my.sharepoint.com (iiitaphyd-my.sharepoint.com)... 13.107.136.9, 13.107.138.9\n",
            "Connecting to iiitaphyd-my.sharepoint.com (iiitaphyd-my.sharepoint.com)|13.107.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-06-12 06:50:29 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RCTW\n",
        "\n",
        "ICDAR2017 Competition on Reading Chinese Text in the Wild"
      ],
      "metadata": {
        "id": "JXFbuwS5YzzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HierText\n",
        "\n",
        "ICDAR2017 Competition on Reading Chinese Text in the Wild\n",
        "\n",
        "\n",
        "Step2: Clone HierText repo to get annotations"
      ],
      "metadata": {
        "id": "YoL5_DIhY1z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMGUR (TBD)"
      ],
      "metadata": {
        "id": "o22xJRjoSYrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imgur(npath):\n",
        "\n",
        "\n",
        "\n",
        "  root=os.path.join(npath,'imgur')\n",
        "  dannot=os.path.join(root,'annotations')\n",
        "  dimg=os.path.join(root,'imgs')\n",
        "\n",
        "  dpath=dict(dt_img=dict(URL = 'XXX',\n",
        "                         fpath=os.path.join(root,'XX'))\n",
        "            )\n",
        "  \n",
        "\n",
        "  for dp in ([root,dannot,dimg]):\n",
        "    ch_make_folder(dp)\n",
        "\n",
        "  logging.info ('Download images from imgur.com. This may take SEVERAL HOURS!')\n",
        " \n",
        "imgur('/content')"
      ],
      "metadata": {
        "id": "4ZOP_YcgSgK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other"
      ],
      "metadata": {
        "id": "WP8zR-INrMhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://rrc.cvc.uab.es/downloads/ch9_training_images.zip --no-check-certificate\n",
        "\n",
        "\n",
        "def check_dw(sfile,url):\n",
        "  if not isfile(sfile):\n",
        "      logging.info(f'The file {sfile} is not availaible, downloading from {url}')\n",
        "      wget.download(url, out=sfile)\n",
        "      # r = requests.get(url, verify=False,stream=True)  \n",
        "      # with open(sfile, 'wb') as f:\n",
        "      #   f.write(r.content)\n",
        "check_dw('ch9_training_images.zip','https://rrc.cvc.uab.es/downloads/ch9_training_images.zip')"
      ],
      "metadata": {
        "id": "dDozyBRtGe-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import requests\n",
        "def download(url, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        response = requests.get(url, stream=True)\n",
        "        total = response.headers.get('content-length')\n",
        "\n",
        "        if total is None:\n",
        "            f.write(response.content)\n",
        "        else:\n",
        "            downloaded = 0\n",
        "            total = int(total)\n",
        "            for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n",
        "                downloaded += len(data)\n",
        "                f.write(data)\n",
        "                done = int(50*downloaded/total)\n",
        "                sys.stdout.write('\\r[{}{}]'.format('█' * done, '.' * (50-done)))\n",
        "                sys.stdout.flush()\n",
        "    sys.stdout.write('\\n')"
      ],
      "metadata": {
        "id": "2PL-RwvufC4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url='https://download.openmmlab.com/mmocr/data/mixture/Syn90k/label.txt'\n",
        "download(url, 'label.txt')"
      ],
      "metadata": {
        "id": "jNKuHz2Gfcde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}