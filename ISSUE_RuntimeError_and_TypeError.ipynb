{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISSUE: RuntimeError and TypeError",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balandongiv/mmocr_tutorial/blob/main/ISSUE_RuntimeError_and_TypeError.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install MMOCR"
      ],
      "metadata": {
        "id": "Sfvz1sywQ9_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies "
      ],
      "metadata": {
        "id": "Tw7u_baQpEUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd ..\n",
        "# Install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\n",
        "\n",
        "# Install mmdetection\n",
        "!pip install mmdet\n",
        "\n",
        "# # Install mmocr\n",
        "!git clone https://github.com/open-mmlab/mmocr.git\n",
        "%cd mmocr\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e ."
      ],
      "outputs": [],
      "metadata": {
        "id": "DwDY3puNNmhe",
        "tags": [
          "outputPrepend"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Installed Dependencies Versions"
      ],
      "metadata": {
        "id": "DY64JCc0pEUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMDetection installation\n",
        "import mmdet\n",
        "print(mmdet.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "import mmcv\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "print(mmcv.__version__)\n",
        "print(get_compiling_cuda_version())\n",
        "print(get_compiler_version())\n",
        "\n",
        "# Check mmocr installation\n",
        "import mmocr\n",
        "print(mmocr.__version__)\n",
        "\n",
        "%cd /mmocr/\n",
        "!ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113 True\n",
            "2.25.0\n",
            "1.5.3\n",
            "11.3\n",
            "GCC 7.3\n",
            "0.6.0\n",
            "/mmocr\n",
            "CITATION.cff  docs\t   mmocr.egg-info   requirements      setup.py\n",
            "configs       LICENSE\t   model-index.yml  requirements.txt  tests\n",
            "demo\t      MANIFEST.in  README.md\t    resources\t      tools\n",
            "docker\t      mmocr\t   README_zh-CN.md  setup.cfg\n"
          ]
        }
      ],
      "metadata": {
        "id": "JABQfPwQN52g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ff812d-55e8-4a7c-b176-6c7091b83a45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config\n",
        "\n",
        "Lets download the config which tailored made to work with Google Colab.\n"
      ],
      "metadata": {
        "id": "DimOYbIGJ_0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1bsEhczfLa5ydltg_oIU3nLOykxCgupQU --output /content/cfg_debug.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuGiRi9-J1c5",
        "outputId": "ba272674-86af-4bc7-ca9e-2cdc6f1647e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bsEhczfLa5ydltg_oIU3nLOykxCgupQU\n",
            "To: /content/cfg_debug.py\n",
            "100% 2.50k/2.50k [00:00<00:00, 4.81MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resume training from the `epoch_6.pth` with a new list of crop `images`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDjdg6L8ziYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import mmcv\n",
        "from mmcv import Config\n",
        "from mmdet.apis import set_random_seed\n",
        "\n",
        "from mmocr.apis import train_detector\n",
        "from mmocr.datasets import build_dataset\n",
        "from mmocr.models import build_detector\n",
        "\n",
        "\n",
        "cfg = Config.fromfile('/content/cfg_debug.py')\n",
        "\n",
        "seed = 0\n",
        "set_random_seed(0, deterministic=False)\n",
        "cfg.gpu_ids = range(1)\n",
        "\n",
        "# Let also limit the training to only 14 epochs\n",
        "\n",
        "# cfg.runner = dict(type='EpochBasedRunner', max_epochs=14) # Default max_epochs is 1200\n",
        "\n",
        "#save the model every 2 epoch\n",
        "# cfg.checkpoint_config = dict(interval=2) # Default the interval was set to 100\n",
        "\n",
        "print(f'Config:\\n{cfg.pretty_text}')\n",
        "\n",
        "# Build dataset\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "#\n",
        "# # Build the detector\n",
        "model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
        "#\n",
        "# # Add an attribute for visualization convenience\n",
        "model.CLASSES = datasets[0].CLASSES\n",
        "\n",
        "# # Create work_dir\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "train_detector(model, datasets, cfg, distributed=False, validate=True)"
      ],
      "metadata": {
        "id": "HUoqVVdRHcG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cff1d6e-a44f-4e9d-cabb-dc59e90c557f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread QueueFeederThread:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
            "    close()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
            "    queue_sem.release()\n",
            "ValueError: semaphore or lock released too many times\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config:\n",
            "log_config = dict(interval=5, hooks=[dict(type='TextLoggerHook')])\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = None\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "opencv_num_threads = 0\n",
            "mp_start_method = 'fork'\n",
            "optimizer = dict(type='SGD', lr=0.007, momentum=0.9, weight_decay=0.0001)\n",
            "optimizer_config = dict(grad_clip=None)\n",
            "lr_config = dict(policy='poly', power=0.9, min_lr=1e-07, by_epoch=True)\n",
            "runner = dict(type='EpochBasedRunner', max_epochs=1200)\n",
            "checkpoint_config = dict(interval=100)\n",
            "model = dict(\n",
            "    type='DBNet',\n",
            "    backbone=dict(\n",
            "        type='mmdet.ResNet',\n",
            "        depth=50,\n",
            "        num_stages=4,\n",
            "        out_indices=(0, 1, 2, 3),\n",
            "        frozen_stages=-1,\n",
            "        norm_cfg=dict(type='BN', requires_grad=True),\n",
            "        norm_eval=False,\n",
            "        style='pytorch',\n",
            "        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),\n",
            "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),\n",
            "        stage_with_dcn=(False, True, True, True)),\n",
            "    neck=dict(\n",
            "        type='FPNC',\n",
            "        in_channels=[256, 512, 1024, 2048],\n",
            "        lateral_channels=256,\n",
            "        asf_cfg=dict(attention_type='ScaleChannelSpatial')),\n",
            "    bbox_head=dict(\n",
            "        type='DBHead',\n",
            "        in_channels=256,\n",
            "        loss=dict(type='DBLoss', alpha=5.0, beta=10.0, bbce_loss=True),\n",
            "        postprocessor=dict(\n",
            "            type='DBPostprocessor', text_repr_type='quad',\n",
            "            epsilon_ratio=0.002)),\n",
            "    train_cfg=None,\n",
            "    test_cfg=None)\n",
            "img_norm_cfg = dict(\n",
            "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
            "train_pipeline_r18 = [\n",
            "    dict(type='LoadImageFromFile', color_type='color_ignore_orientation'),\n",
            "    dict(\n",
            "        type='LoadTextAnnotations',\n",
            "        with_bbox=True,\n",
            "        with_mask=True,\n",
            "        poly2mask=False),\n",
            "    dict(type='ColorJitter', brightness=0.12549019607843137, saturation=0.5),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[123.675, 116.28, 103.53],\n",
            "        std=[58.395, 57.12, 57.375],\n",
            "        to_rgb=True),\n",
            "    dict(\n",
            "        type='ImgAug',\n",
            "        args=[['Fliplr', 0.5], {\n",
            "            'cls': 'Affine',\n",
            "            'rotate': [-10, 10]\n",
            "        }, ['Resize', [0.5, 3.0]]]),\n",
            "    dict(type='EastRandomCrop', target_size=(640, 640)),\n",
            "    dict(type='DBNetTargets', shrink_ratio=0.4),\n",
            "    dict(type='Pad', size_divisor=32),\n",
            "    dict(\n",
            "        type='CustomFormatBundle',\n",
            "        keys=['gt_shrink', 'gt_shrink_mask', 'gt_thr', 'gt_thr_mask'],\n",
            "        visualize=dict(flag=False, boundary_key='gt_shrink')),\n",
            "    dict(\n",
            "        type='Collect',\n",
            "        keys=['img', 'gt_shrink', 'gt_shrink_mask', 'gt_thr', 'gt_thr_mask'])\n",
            "]\n",
            "test_pipeline_1333_736 = [\n",
            "    dict(type='LoadImageFromFile', color_type='color_ignore_orientation'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(1333, 736),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "img_norm_cfg_r50dcnv2 = dict(\n",
            "    mean=[122.67891434, 116.66876762, 104.00698793],\n",
            "    std=[58.395, 57.12, 57.375],\n",
            "    to_rgb=True)\n",
            "train_pipeline_r50dcnv2 = [\n",
            "    dict(type='LoadImageFromFile', color_type='color_ignore_orientation'),\n",
            "    dict(\n",
            "        type='LoadTextAnnotations',\n",
            "        with_bbox=True,\n",
            "        with_mask=True,\n",
            "        poly2mask=False),\n",
            "    dict(type='ColorJitter', brightness=0.12549019607843137, saturation=0.5),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[122.67891434, 116.66876762, 104.00698793],\n",
            "        std=[58.395, 57.12, 57.375],\n",
            "        to_rgb=True),\n",
            "    dict(\n",
            "        type='ImgAug',\n",
            "        args=[['Fliplr', 0.5], {\n",
            "            'cls': 'Affine',\n",
            "            'rotate': [-10, 10]\n",
            "        }, ['Resize', [0.5, 3.0]]]),\n",
            "    dict(type='EastRandomCrop', target_size=(640, 640)),\n",
            "    dict(type='DBNetTargets', shrink_ratio=0.4),\n",
            "    dict(type='Pad', size_divisor=32),\n",
            "    dict(\n",
            "        type='CustomFormatBundle',\n",
            "        keys=['gt_shrink', 'gt_shrink_mask', 'gt_thr', 'gt_thr_mask'],\n",
            "        visualize=dict(flag=False, boundary_key='gt_shrink')),\n",
            "    dict(\n",
            "        type='Collect',\n",
            "        keys=['img', 'gt_shrink', 'gt_shrink_mask', 'gt_thr', 'gt_thr_mask'])\n",
            "]\n",
            "test_pipeline_4068_1024 = [\n",
            "    dict(type='LoadImageFromFile', color_type='color_ignore_orientation'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(4068, 1024),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[122.67891434, 116.66876762, 104.00698793],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "root = '/mmocr/tests/data/toy_dataset'\n",
            "work_dir = '/mmocr/resources/train_detect'\n",
            "test = dict(\n",
            "    type='TextDetDataset',\n",
            "    img_prefix='/mmocr/tests/data/toy_dataset/imgs',\n",
            "    ann_file='/mmocr/tests/data/toy_dataset/instances_test.txt',\n",
            "    loader=dict(\n",
            "        type='AnnFileLoader',\n",
            "        repeat=1,\n",
            "        file_format='txt',\n",
            "        parser=dict(\n",
            "            type='LineJsonParser',\n",
            "            keys=['file_name', 'height', 'width', 'annotations'])),\n",
            "    pipeline=None,\n",
            "    test_mode=True)\n",
            "train_list = [\n",
            "    dict(\n",
            "        type='TextDetDataset',\n",
            "        img_prefix='/mmocr/tests/data/toy_dataset/imgs',\n",
            "        ann_file='/mmocr/tests/data/toy_dataset/instances_test.txt',\n",
            "        loader=dict(\n",
            "            type='AnnFileLoader',\n",
            "            repeat=1,\n",
            "            file_format='txt',\n",
            "            parser=dict(\n",
            "                type='LineJsonParser',\n",
            "                keys=['file_name', 'height', 'width', 'annotations'])),\n",
            "        pipeline=None,\n",
            "        test_mode=True)\n",
            "]\n",
            "test_list = [\n",
            "    dict(\n",
            "        type='TextDetDataset',\n",
            "        img_prefix='/mmocr/tests/data/toy_dataset/imgs',\n",
            "        ann_file='/mmocr/tests/data/toy_dataset/instances_test.txt',\n",
            "        loader=dict(\n",
            "            type='AnnFileLoader',\n",
            "            repeat=1,\n",
            "            file_format='txt',\n",
            "            parser=dict(\n",
            "                type='LineJsonParser',\n",
            "                keys=['file_name', 'height', 'width', 'annotations'])),\n",
            "        pipeline=None,\n",
            "        test_mode=True)\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=4,\n",
            "    workers_per_gpu=4,\n",
            "    val_dataloader=dict(samples_per_gpu=1),\n",
            "    test_dataloader=dict(samples_per_gpu=1),\n",
            "    train=dict(\n",
            "        type='UniformConcatDataset',\n",
            "        datasets=[\n",
            "            dict(\n",
            "                type='TextDetDataset',\n",
            "                img_prefix='/mmocr/tests/data/toy_dataset/imgs',\n",
            "                ann_file='/mmocr/tests/data/toy_dataset/instances_test.txt',\n",
            "                loader=dict(\n",
            "                    type='AnnFileLoader',\n",
            "                    repeat=1,\n",
            "                    file_format='txt',\n",
            "                    parser=dict(\n",
            "                        type='LineJsonParser',\n",
            "                        keys=['file_name', 'height', 'width', 'annotations'])),\n",
            "                pipeline=None,\n",
            "                test_mode=True)\n",
            "        ],\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                type='LoadImageFromFile',\n",
            "                color_type='color_ignore_orientation'),\n",
            "            dict(\n",
            "                type='LoadTextAnnotations',\n",
            "                with_bbox=True,\n",
            "                with_mask=True,\n",
            "                poly2mask=False),\n",
            "            dict(\n",
            "                type='ColorJitter',\n",
            "                brightness=0.12549019607843137,\n",
            "                saturation=0.5),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[122.67891434, 116.66876762, 104.00698793],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(\n",
            "                type='ImgAug',\n",
            "                args=[['Fliplr', 0.5], {\n",
            "                    'cls': 'Affine',\n",
            "                    'rotate': [-10, 10]\n",
            "                }, ['Resize', [0.5, 3.0]]]),\n",
            "            dict(type='EastRandomCrop', target_size=(640, 640)),\n",
            "            dict(type='DBNetTargets', shrink_ratio=0.4),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(\n",
            "                type='CustomFormatBundle',\n",
            "                keys=['gt_shrink', 'gt_shrink_mask', 'gt_thr', 'gt_thr_mask'],\n",
            "                visualize=dict(flag=False, boundary_key='gt_shrink')),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=[\n",
            "                    'img', 'gt_shrink', 'gt_shrink_mask', 'gt_thr',\n",
            "                    'gt_thr_mask'\n",
            "                ])\n",
            "        ]),\n",
            "    val=dict(\n",
            "        type='UniformConcatDataset',\n",
            "        datasets=[\n",
            "            dict(\n",
            "                type='TextDetDataset',\n",
            "                img_prefix='/mmocr/tests/data/toy_dataset/imgs',\n",
            "                ann_file='/mmocr/tests/data/toy_dataset/instances_test.txt',\n",
            "                loader=dict(\n",
            "                    type='AnnFileLoader',\n",
            "                    repeat=1,\n",
            "                    file_format='txt',\n",
            "                    parser=dict(\n",
            "                        type='LineJsonParser',\n",
            "                        keys=['file_name', 'height', 'width', 'annotations'])),\n",
            "                pipeline=None,\n",
            "                test_mode=True)\n",
            "        ],\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                type='LoadImageFromFile',\n",
            "                color_type='color_ignore_orientation'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(4068, 1024),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[122.67891434, 116.66876762, 104.00698793],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]),\n",
            "    test=dict(\n",
            "        type='UniformConcatDataset',\n",
            "        datasets=[\n",
            "            dict(\n",
            "                type='TextDetDataset',\n",
            "                img_prefix='/mmocr/tests/data/toy_dataset/imgs',\n",
            "                ann_file='/mmocr/tests/data/toy_dataset/instances_test.txt',\n",
            "                loader=dict(\n",
            "                    type='AnnFileLoader',\n",
            "                    repeat=1,\n",
            "                    file_format='txt',\n",
            "                    parser=dict(\n",
            "                        type='LineJsonParser',\n",
            "                        keys=['file_name', 'height', 'width', 'annotations'])),\n",
            "                pipeline=None,\n",
            "                test_mode=True)\n",
            "        ],\n",
            "        pipeline=[\n",
            "            dict(\n",
            "                type='LoadImageFromFile',\n",
            "                color_type='color_ignore_orientation'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(4068, 1024),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[122.67891434, 116.66876762, 104.00698793],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]))\n",
            "evaluation = dict(interval=100, metric='hmean-iou')\n",
            "gpu_ids = range(0, 1)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2022-07-02 13:47:11,672 - mmocr - INFO - Start running, host: root@13a424f37129, work_dir: /mmocr/resources/train_detect\n",
            "2022-07-02 13:47:11,674 - mmocr - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(NORMAL      ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(NORMAL      ) EvalHook                           \n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
            "(NORMAL      ) EvalHook                           \n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(ABOVE_NORMAL) OptimizerHook                      \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(NORMAL      ) EvalHook                           \n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(NORMAL      ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_run:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "2022-07-02 13:47:11,677 - mmocr - INFO - workflow: [('train', 1)], max: 1200 epochs\n",
            "2022-07-02 13:47:11,683 - mmocr - INFO - Checkpoints will be saved to /mmocr/resources/train_detect by HardDiskBackend.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7d556c238058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# # Create work_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrain_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/mmocr/mmocr/apis/train.py\u001b[0m in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0mepoch_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Prevent possible deadlock during epoch transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 416, in __getitem__\n    return self.datasets[dataset_idx][sample_idx]\n  File \"/mmocr/mmocr/datasets/base_dataset.py\", line 139, in __getitem__\n    return self.prepare_test_img(index)\n  File \"/mmocr/mmocr/datasets/base_dataset.py\", line 111, in prepare_test_img\n    return self.prepare_train_img(img_info)\n  File \"/mmocr/mmocr/datasets/text_det_dataset.py\", line 78, in prepare_train_img\n    return self.pipeline(results)\n  File \"/usr/local/lib/python3.7/dist-packages/mmdet/datasets/pipelines/compose.py\", line 41, in __call__\n    data = t(data)\n  File \"/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py\", line 83, in __call__\n    self.may_augment_annotation(aug, shape, target_shape, results)\n  File \"/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py\", line 94, in may_augment_annotation\n    masks = self.may_augment_poly(aug, shape, results[key])\n  File \"/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py\", line 140, in may_augment_poly\n    for point in poly:\nTypeError: 'Polygon' object is not iterable\n"
          ]
        }
      ]
    }
  ]
}