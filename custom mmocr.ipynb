{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom mmocr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balandongiv/mmocr_tutorial/blob/main/custom%20mmocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMOCR Tutorial\n",
        "\n",
        "Welcome to MMOCR! This is the **unofficial** colab tutorial for using MMOCR. In this tutorial, you will learn how to\n",
        "\n",
        "- Transform annotation in coco-like json format to MMOCR compatible format.\n",
        "- Set custom configuration file\n",
        "- Integrate mmocr config template extracted from `base` with user custom setting (Not sure if this is possible? or how to achieve:KIV)\n",
        "- Train a text recognizer with a toy dataset. "
      ],
      "metadata": {
        "id": "U-Xyj4cQrEQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install MMOCR"
      ],
      "metadata": {
        "id": "Sfvz1sywQ9_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When installing dependencies for mmocr, please ensure that all the dependency versions are compatible with each other. For instance, if CUDA 10.1 is installed, then the Pytorch version must be compatible with cu10.1. Please see [getting_started.md](docs/getting_started.md) for more details. "
      ],
      "metadata": {
        "id": "q3fZP1LspEUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd .."
      ],
      "outputs": [],
      "metadata": {
        "id": "rB3qciTXpEUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check PyTorch version"
      ],
      "metadata": {
        "id": "mSkZOdrMpEUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "id": "eNvnx5XGAn0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies "
      ],
      "metadata": {
        "id": "Tw7u_baQpEUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\n",
        "\n",
        "# Install mmdetection\n",
        "!pip install mmdet\n",
        "\n",
        "# Install mmocr\n",
        "!git clone https://github.com/open-mmlab/mmocr.git\n",
        "%cd mmocr\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e ."
      ],
      "outputs": [],
      "metadata": {
        "id": "DwDY3puNNmhe",
        "tags": [
          "outputPrepend"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Installed Dependencies Versions"
      ],
      "metadata": {
        "id": "DY64JCc0pEUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMDetection installation\n",
        "import mmdet\n",
        "print(mmdet.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "import mmcv\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "print(mmcv.__version__)\n",
        "print(get_compiling_cuda_version())\n",
        "print(get_compiler_version())\n",
        "\n",
        "# Check mmocr installation\n",
        "import mmocr\n",
        "print(mmocr.__version__)\n",
        "\n",
        "%cd /mmocr/\n",
        "!ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "JABQfPwQN52g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform Training on a Toy Dataset with MMOCR Recognizer\n",
        "We now demonstrate how to perform training with an MMOCR recognizer. Since training a full academic dataset is time consuming (usually takes about several hours), we will train on the toy dataset for the SAR text recognition model and visualize the predictions.\n",
        "\n",
        "Training a dataset usually consists of the following steps:\n",
        "1. Convert the dataset into a format supported by MMOCR (e.g. COCO for text detection). The annotation file can be in either .txt or .lmdb format, depending on the size of the dataset. This step is usually applicable to customized datasets, since the datasets and annotation files we provide are already in supported formats. \n",
        "2. Modify the config for training. \n",
        "3. Train the model. \n",
        "\n"
      ],
      "metadata": {
        "id": "nYon41X7RTOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the Toy Dataset\n",
        "\n",
        "In this tutorial, we will be working with the dataset under the folder `toy_dataset`. \n",
        "\n",
        "Each of the image have the accompnying `annotation` file generated using the `labelme`\n",
        "\n",
        "The images were stored under the following path\n",
        "\n",
        "`/mmocr/tests/data/toy_dataset/imgs/test`\n",
        "\n",
        "The annotation were stored under  the following path\n",
        "\n",
        "`/mmocr/tests/data/toy_dataset/labelme`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FElJSp1vpEUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print list of images and annotation available under the`toy_dataset` folder\n",
        "!cat /mmocr/tests/data/toy_dataset/img_list.txt"
      ],
      "metadata": {
        "id": "FzyOZkrwCxEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first get a sense of what the toy dataset looks like by visualizing one of the images and labels. "
      ],
      "metadata": {
        "id": "qLEON60YC1Lj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import mmcv\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#To ensure we are in the root directory\n",
        "%cd .. \n",
        "\n",
        "img = mmcv.imread('/mmocr/tests/data/toy_dataset/imgs/test/img_1.jpg')\n",
        "\n",
        "plt.imshow(mmcv.bgr2rgb(img))\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "hZfd2pnqN5-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let visualise the annotation"
      ],
      "metadata": {
        "id": "O1V3V_4L_DHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /mmocr/tests/data/toy_dataset/labelme/img_1.json"
      ],
      "metadata": {
        "id": "ZP6Y4nLbFsYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Annotation to MMOCR format \n",
        "\n",
        "Since the annotations are in COCO format, we need to firstly convert annotation in labelme format to MMOCR's annotation format using the `labelme_converter.py`.\n",
        "\n",
        "For this particular example, we will convert the labelme format annotation in `/mmocr/tests/data/toy_dataset/labelme` to MMOCR detection labels instances_training `.txt` and export the cropped image patches for recognition task to `/mmocr/tests/data/toy_dataset/crops`. It is worth to note that the following setting would not output the `test_label.txt` as there is no test set. This information is important  when assigning  `test_anno_file1` to `train_label.txt` in subsequent step.\n",
        "\n",
        "In Google colab, it is essential to add a `!` before the Python command.\n"
      ],
      "metadata": {
        "id": "HDjdg6L8ziYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /mmocr/tools/data/common/labelme_converter.py /mmocr/tests/data/toy_dataset/labelme /mmocr/tests/data/toy_dataset/imgs/test /mmocr/tests/data/toy_dataset --tasks recog --format txt"
      ],
      "metadata": {
        "id": "TT6LyieYrt-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let have a look what files has been generated from the previous calling"
      ],
      "metadata": {
        "id": "NVsw-hMkETbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of crop images that being crop in accordance to the bounding box\n",
        "import os\n",
        "print([os.path.join(path, name) for path, subdirs, files in os.walk('/mmocr/tests/data/toy_dataset/crops') for name in files])"
      ],
      "metadata": {
        "id": "X8kvWR3UE4KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets visualise the MMOCR detection labels instances_training `.txt`"
      ],
      "metadata": {
        "id": "VmsBm8txF06t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /mmocr/tests/data/toy_dataset/train_label.txt"
      ],
      "metadata": {
        "id": "RFhKFgJZGJW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /mmocr/tests/data/toy_dataset/instances_training.txt"
      ],
      "metadata": {
        "id": "o6MGWUg4GHDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration file\n",
        "\n",
        "The MMOCR's config system incorporate modular and inheritance design into our, which is convenient to conduct various experiments. \n",
        "\n",
        "Lets visualise the `sar_r31_parallel_decoder_toy_dataset` config."
      ],
      "metadata": {
        "id": "i-GrV0xSkAc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mmcv import Config\n",
        "\n",
        "# Note the addition of /mmocr preceding text\n",
        "cfg = Config.fromfile('/mmocr/configs/textrecog/sar/sar_r31_parallel_decoder_toy_dataset.py') \n",
        "print(f'Config:\\n{cfg.pretty_text}')"
      ],
      "metadata": {
        "id": "VO3_9X_6Yaxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MMOCR have the build-in facility to generate the configfor config and config files.\n",
        "\n",
        "The config file structure consist on the parameter `model`, `data & pipeline`, `training schedule`, and `runtime setting`. Please refer to [config file structure](https://mmocr.readthedocs.io/en/latest/tutorials/config.html#config-file-structure) for detailed documentation"
      ],
      "metadata": {
        "id": "NvU8d9q-ZbcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set custom configuration file. Most of the values are based on \n",
        " `sar_r31_parallel_decoder_toy_dataset` config value."
      ],
      "metadata": {
        "id": "uzra5ObPagF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log_config = dict(interval = 40, hooks = [dict(type = 'TextLoggerHook')]) # Purposely disable and tweak in next cell. Due to Colab consideration\n",
        "dist_params = dict(backend = 'nccl')\n",
        "log_level = 'INFO'\n",
        "\n",
        "load_from = None\n",
        "\n",
        "resume_from = None\n",
        "\n",
        "workflow = [('train', 1)]\n",
        "\n",
        "opencv_num_threads = 0\n",
        "\n",
        "mp_start_method = 'fork'\n",
        "\n",
        "label_convertor = dict(type = 'AttnConvertor', \n",
        "                       dict_type = 'DICT90', \n",
        "                       with_unknown = True)\n",
        "\n",
        "model = dict(type = 'SARNet',\n",
        "            backbone = dict(type = 'ResNet31OCR'),\n",
        "            encoder = dict(type = 'SAREncoder', \n",
        "                           enc_bi_rnn = False, \n",
        "                           enc_do_rnn = 0.1, \n",
        "                           enc_gru = False),\n",
        "            decoder = dict(type = 'ParallelSARDecoder',\n",
        "                          enc_bi_rnn = False,\n",
        "                          dec_bi_rnn = False,\n",
        "                          dec_do_rnn = 0,\n",
        "                          dec_gru = False,\n",
        "                          pred_dropout = 0.1,\n",
        "                          d_k = 512,\n",
        "                          pred_concat = True),\n",
        "          loss = dict(type = 'SARLoss'),\n",
        "          label_convertor = dict(type = 'AttnConvertor', \n",
        "                               dict_type = 'DICT90', \n",
        "                               with_unknown = True),\n",
        "          max_seq_len = 30)\n",
        "\n",
        "# optimizer = dict(type = 'Adam', lr = 0.000125)  # Purposely disable and tweak in next cell. Due to Colab consideration\n",
        "optimizer_config = dict(grad_clip = None)\n",
        "# lr_config = dict(policy = 'step', step = [3, 4], warmup = None) # Purposely disable and tweak in next cell. Due to Colab consideration\n",
        "\n",
        "\n",
        "\n",
        "runner = dict(type = 'EpochBasedRunner', \n",
        "              max_epochs = 5)             # For speed consideration, we limit number of epochs to only 5\n",
        "\n",
        "checkpoint_config = dict(interval = 1)\n",
        "\n",
        "img_norm_cfg = dict(mean = [0.5, 0.5, 0.5], \n",
        "                    std = [0.5, 0.5, 0.5])\n",
        "\n",
        "train_pipeline = [                                            # Training pipeline\n",
        "                  dict(type = 'LoadImageFromFile'),           # First pipeline to load images from file path\n",
        "                  dict(type = 'ResizeOCR',\n",
        "                      height = 48,\n",
        "                      min_width = 48,\n",
        "                      max_width = 160,\n",
        "                      keep_aspect_ratio = True,\n",
        "                      width_downsample_ratio = 0.25),\n",
        "                  dict(type = 'ToTensorOCR'),\n",
        "                  dict(type = 'NormalizeOCR', \n",
        "                       mean = [0.5, 0.5, 0.5], \n",
        "                       std = [0.5, 0.5, 0.5]),\n",
        "                  dict(type = 'Collect',\n",
        "                      keys = ['img'],\n",
        "                      meta_keys = ['filename', 'ori_shape', \n",
        "                                   'resize_shape',  'text', 'valid_ratio'])\n",
        "                  ]\n",
        "\n",
        "\n",
        "# Parameter for test\n",
        "transform_test_pl=[dict(\n",
        "                    type = 'ResizeOCR',\n",
        "                    height = 48,\n",
        "                    min_width = 48,\n",
        "                    max_width = 160,\n",
        "                    keep_aspect_ratio = True,\n",
        "                    width_downsample_ratio = 0.25),\n",
        "                  dict(type = 'ToTensorOCR'),\n",
        "                  dict(type = 'NormalizeOCR', \n",
        "                       mean = [0.5, 0.5, 0.5], \n",
        "                       std = [0.5, 0.5,0.5]),\n",
        "                dict(type = 'Collect',\n",
        "                    keys = ['img'],\n",
        "                    meta_keys = ['filename', 'ori_shape', \n",
        "                                 'resize_shape', 'valid_ratio',\n",
        "                                 'img_norm_cfg', 'ori_filename', \n",
        "                                 'img_shape'\n",
        "                                ])]\n",
        "\n",
        "\n",
        "test_pipeline = [ dict(type = 'LoadImageFromFile'),\n",
        "                  dict(type = 'MultiRotateAugOCR',\n",
        "                  rotate_degrees = [0, 90, 270],\n",
        "                  transforms = transform_test_pl)\n",
        "                ]\n",
        "\n",
        "evaluation = dict(interval = 1, \n",
        "                  metric = 'acc')\n",
        "\n"
      ],
      "metadata": {
        "id": "Kqvdt6KchPZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the path reflecting where the crop images and annotation being saved"
      ],
      "metadata": {
        "id": "dV2_VCGOhZ7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_type = 'OCRDataset'\n",
        "root = '/mmocr/tests/data/toy_dataset'\n",
        "img_prefix = '/mmocr/tests/data/toy_dataset/crops'\n",
        "train_anno_file1 = '/mmocr/tests/data/toy_dataset/train_label.txt'\n",
        "\n",
        "test_anno_file1 = '/mmocr/tests/data/toy_dataset/train_label.txt'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loader_dt_train = dict(type='AnnFileLoader',\n",
        "                            repeat=100,\n",
        "                            file_format='txt',\n",
        "                            file_storage_backend='disk',\n",
        "                            parser=dict(type='LineStrParser',\n",
        "                                        keys=['filename', 'text'],\n",
        "                                        keys_idx=[0, 1],\n",
        "                                        separator=' '))\n",
        "\n",
        "train_datasets1 = dict(type='OCRDataset',\n",
        "                       img_prefix=img_prefix,\n",
        "                       ann_file=train_anno_file1,\n",
        "                       loader=loader_dt_train,\n",
        "                       pipeline=None,           # why the pipeline is set to None?\n",
        "                       test_mode=False)\n",
        "\n",
        "\n",
        "loader_dt_test = dict(type = 'AnnFileLoader',\n",
        "                        repeat = 1,\n",
        "                        file_format = 'txt',\n",
        "                        file_storage_backend = 'disk',\n",
        "                        parser = dict(type = 'LineStrParser',\n",
        "                                    keys = ['filename', 'text'])) # why no keys_idx and separator here?\n",
        "\n",
        "# For time being, assume the setting for training and validation is the same, though practically we separate testing sets\n",
        "val_dataset = dict(type='OCRDataset',\n",
        "                   img_prefix=img_prefix,\n",
        "                   ann_file=train_anno_file1,\n",
        "                   loader=loader_dt_test,\n",
        "                   pipeline=None,               # why the pipeline is set to None?\n",
        "                   test_mode=True)\n",
        "\n",
        "## It is possible to have two list [train1, train2]\n",
        "train_list = [train_datasets1]\n",
        "test_list = [val_dataset]\n",
        "\n",
        "transformer_val = [dict(type='ResizeOCR',\n",
        "                        height=48,\n",
        "                        min_width=48,\n",
        "                        max_width=160,\n",
        "                        keep_aspect_ratio=True,\n",
        "                        width_downsample_ratio=0.25),\n",
        "                   dict(type='ToTensorOCR'),\n",
        "                   dict(type='NormalizeOCR',\n",
        "                        mean=[0.5, 0.5, 0.5],\n",
        "                        std=[0.5, 0.5, 0.5]),\n",
        "                   dict(type='Collect',\n",
        "                        keys=['img'],\n",
        "                        meta_keys=[\n",
        "                            'filename', 'ori_shape', 'resize_shape',\n",
        "                            'valid_ratio', 'img_norm_cfg', 'ori_filename',\n",
        "                            'img_shape'\n",
        "                        ])\n",
        "                   ]\n",
        "\n",
        "data_test_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(\n",
        "        type='MultiRotateAugOCR',\n",
        "        rotate_degrees=[0, 90, 270],\n",
        "        transforms=[\n",
        "            dict(\n",
        "                type='ResizeOCR',\n",
        "                height=48,\n",
        "                min_width=48,\n",
        "                max_width=160,\n",
        "                keep_aspect_ratio=True,\n",
        "                width_downsample_ratio=0.25),\n",
        "            dict(type='ToTensorOCR'),\n",
        "            dict(\n",
        "                type='NormalizeOCR',\n",
        "                mean=[0.5, 0.5, 0.5],\n",
        "                std=[0.5, 0.5, 0.5]),\n",
        "            dict(\n",
        "                type='Collect',\n",
        "                keys=['img'],\n",
        "                meta_keys=[\n",
        "                    'filename', 'ori_shape', 'resize_shape',\n",
        "                    'valid_ratio', 'img_norm_cfg', 'ori_filename',\n",
        "                    'img_shape'\n",
        "                ])\n",
        "        ])\n",
        "]\n",
        "\n",
        "data_test_datasets = [\n",
        "    dict(\n",
        "        type='OCRDataset',\n",
        "        img_prefix=img_prefix,\n",
        "        ann_file=train_anno_file1,\n",
        "        loader=dict(\n",
        "            type='AnnFileLoader',\n",
        "            repeat=1,\n",
        "            file_format='txt',\n",
        "            file_storage_backend='disk',\n",
        "            parser=dict(\n",
        "                type='LineStrParser',\n",
        "                keys=['filename', 'text'])),\n",
        "        pipeline=None,\n",
        "        test_mode=True)\n",
        "]\n",
        "\n",
        "data_val = dict(type='UniformConcatDataset',\n",
        "                datasets=[val_dataset],\n",
        "                pipeline=[dict(type='LoadImageFromFile'),\n",
        "                          dict(type='MultiRotateAugOCR',\n",
        "                               rotate_degrees=[0, 90, 270],\n",
        "                               transforms=transformer_val)\n",
        "                          ])\n",
        "\n",
        "data_train = dict(  # train data config\n",
        "    type='UniformConcatDataset',\n",
        "    datasets=[train_datasets1], # It is possible to have two train datasets here\n",
        "    pipeline=[\n",
        "        dict(type='LoadImageFromFile'),\n",
        "        dict(\n",
        "            type='ResizeOCR',\n",
        "            height=48,\n",
        "            min_width=48,\n",
        "            max_width=160,\n",
        "            keep_aspect_ratio=True,\n",
        "            width_downsample_ratio=0.25),\n",
        "        dict(type='ToTensorOCR'),\n",
        "        dict(type='NormalizeOCR',\n",
        "             mean=[0.5, 0.5, 0.5],\n",
        "             std=[0.5, 0.5, 0.5]),\n",
        "        dict(type='Collect',\n",
        "             keys=['img'],\n",
        "             meta_keys=[\n",
        "                 'filename', 'ori_shape', 'resize_shape', 'text',\n",
        "                 'valid_ratio'\n",
        "             ])\n",
        "    ])\n",
        "\n",
        "data = dict(\n",
        "    workers_per_gpu=2,  # Worker to pre-fetch data for each single GPU\n",
        "    samples_per_gpu=8,  # Batch size of a single GPU\n",
        "    train=data_train,\n",
        "    val=data_val,\n",
        "    test=dict(type='UniformConcatDataset',\n",
        "              datasets=data_test_datasets,\n",
        "              pipeline=data_test_pipeline))\n"
      ],
      "metadata": {
        "id": "VeVIW6_ThWAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform inference for SAR on colab, we need to adjust the config values to accommodate some of the settings of colab such as the number of GPU available. "
      ],
      "metadata": {
        "id": "AOLdCnfeYlu0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from mmdet.apis import set_random_seed\n",
        "\n",
        "# Set up working dir to save files and logs.\n",
        "work_dir = '/mmocr/demo/tutorial_exps'\n",
        "\n",
        "# The original learning rate (LR) is set for 8-GPU training.\n",
        "# We divide it by 8 since we only use one GPU.\n",
        "\n",
        "optimizer = dict(type = 'Adam', \n",
        "                 lr = 0.001 / 8)\n",
        "\n",
        "\n",
        "lr_config = dict(policy = 'step', \n",
        "                 step = [3, 4], \n",
        "                 warmup = None  # Set warmup to None when work in Colab\n",
        "                 ) \n",
        "\n",
        "\n",
        "log_config = dict(\n",
        "                  interval = 40,  # Choose to log training results every 40 images to reduce the size of log file. \n",
        "                  hooks = [dict(type = 'TextLoggerHook')])\n",
        "\n",
        "# Set seed thus the results are more reproducible\n",
        "seed = 0\n",
        "set_random_seed(0, deterministic=False)\n",
        "gpu_ids = range(1)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "67OJ6oAvN6NA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create the configration file"
      ],
      "metadata": {
        "id": "Tp2_REBignfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mmcv import Config\n",
        "\n",
        "\n",
        "cfg = Config(dict(log_config = log_config,\n",
        "                  dist_params=dist_params,\n",
        "                  log_level =log_level,\n",
        "                  load_from=load_from,\n",
        "                  resume_from=resume_from,\n",
        "                  workflow=workflow,\n",
        "                  opencv_num_threads=opencv_num_threads,\n",
        "                  mp_start_method=mp_start_method,\n",
        "                  label_convertor=label_convertor,\n",
        "                  model=model,\n",
        "                  optimizer=optimizer,                # Specially tweak for Google Colab particularly the  lr\n",
        "                  optimizer_config=optimizer_config,\n",
        "                  lr_config=lr_config,\n",
        "                  runner=runner,\n",
        "                  checkpoint_config=checkpoint_config,\n",
        "                  img_norm_cfg=img_norm_cfg,\n",
        "                  train_pipeline=train_pipeline,\n",
        "                  test_pipeline=test_pipeline,\n",
        "                  dataset_type=dataset_type,\n",
        "                  root=root,\n",
        "                  img_prefix=img_prefix,\n",
        "                  train_anno_file1=train_anno_file1,\n",
        "                  train1=train1,\n",
        "                  test_anno_file1=test_anno_file1,\n",
        "                  test=test,                            # Disable as this is for 1mdb\n",
        "                  train_list=train_list,\n",
        "                  test_list=test_list,\n",
        "                  data=data,\n",
        "                  evaluation=evaluation,\n",
        "                  work_dir=work_dir,\n",
        "                  seed =seed,                           # Specially tweak for Google Colab\n",
        "                  gpu_ids =gpu_ids,                     # Specially tweak for Google Colab\n",
        "                  ))\n",
        "\n",
        "\n",
        "print(f'Config:\\n{cfg.pretty_text}')"
      ],
      "metadata": {
        "id": "XUzHEOHBVWr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the SAR Text Recognizer \n",
        "Finally, we train the SAR text recognizer on the toy dataset for five epochs. \n",
        "\n",
        "This steps took about ~ 5 minutes to complete"
      ],
      "metadata": {
        "id": "TZj5vyqEmulE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mmocr.datasets import build_dataset\n",
        "from mmocr.models import build_detector\n",
        "from mmocr.apis import train_detector\n",
        "import os.path as osp\n",
        "\n",
        "# Build dataset\n",
        "datasets = [build_dataset(cfg.data.train)] \n",
        "\n",
        "# Build the detector\n",
        "model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
        "\n",
        "# Add an attribute for visualization convenience\n",
        "model.CLASSES = datasets[0].CLASSES\n",
        "\n",
        "# Create work_dir\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "train_detector(model, datasets, cfg, distributed=False, validate=True)"
      ],
      "metadata": {
        "id": "aGtuC29ys8Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test and Visualize the Predictions\n",
        "\n",
        "For completeness, we also perform testing on the latest checkpoint and evaluate the results with hmean-iou metrics. The predictions are saved in the ./outputs file. "
      ],
      "metadata": {
        "id": "sklydRNXnfJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the testing image\n",
        "img='/mmocr/tests/data/toy_dataset/imgs/test/img_1.jpg'\n",
        "# img = '/mmocr/tests/data/ocr_toy_dataset/imgs/1036169.jpg' # original value > './tests/data/ocr_toy_dataset/imgs/1036169.jpg'\n",
        "_img = mmcv.imread(img)\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(mmcv.bgr2rgb(_img))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UcaZm7qY3yGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from mmocr.apis import init_detector, model_inference\n",
        "# work_dir = '/mmocr/demo/tutorial_exps' # To recap, we set the working dir\n",
        "img='/mmocr/tests/data/toy_dataset/imgs/test/img_1.jpg'\n",
        "# img = '/mmocr/tests/data/ocr_toy_dataset/imgs/1036169.jpg' # original value > './tests/data/ocr_toy_dataset/imgs/1036169.jpg'\n",
        "checkpoint = \"/mmocr/demo/tutorial_exps/epoch_5.pth\"   # Original value \"./demo/tutorial_exps/epoch_5.pth\" > please expect error\n",
        "out_file = 'outputs/1036169s.jpg'\n",
        "\n",
        "model = init_detector(cfg, checkpoint, device=\"cuda:0\")\n",
        "if model.cfg.data.test['type'] == 'ConcatDataset':\n",
        "    model.cfg.data.test.pipeline = model.cfg.data.test['datasets'][0].pipeline\n",
        "\n",
        "\n",
        "result = model_inference(model, img)\n",
        "print(f'result: {result}')\n",
        "\n",
        "img = model.show_result(img, result, \n",
        "                        out_file=out_file, \n",
        "                        show=False)\n",
        "\n",
        "mmcv.imwrite(img, out_file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-HbXY7uUpEU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Visualize the results\n",
        "predicted_img = mmcv.imread('./outputs/1036169s.jpg')\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(mmcv.bgr2rgb(predicted_img))\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "k3s27QIGQCnT"
      }
    }
  ]
}